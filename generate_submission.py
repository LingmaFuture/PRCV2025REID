#!/usr/bin/env python3
"""
PRCV2025å…¨æ¨¡æ€è¡Œäººé‡è¯†åˆ«ç«èµ›æäº¤æ–‡ä»¶ç”Ÿæˆå™¨

ç”Ÿæˆç¬¦åˆæ¯”èµ›è¦æ±‚çš„CSVæäº¤æ–‡ä»¶ï¼Œæ”¯æŒï¼š
- å•æ¨¡æ€æŸ¥è¯¢: onemodal_NIR, onemodal_SK, onemodal_CP, onemodal_TEXT
- åŒæ¨¡æ€æŸ¥è¯¢: twomodal_*
- ä¸‰æ¨¡æ€æŸ¥è¯¢: threemodal_*  
- å››æ¨¡æ€æŸ¥è¯¢: fourmodal_*
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import ast

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.amp import autocast

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from models.model import MultiModalReIDModel
from datasets.dataset import MultiModalDataset, compatible_collate_fn
from configs.config import TrainingConfig


class PRCV2025SubmissionGenerator:
    """PRCV2025ç«èµ›æäº¤æ–‡ä»¶ç”Ÿæˆå™¨"""
    
    def __init__(self, config_path: str = None, model_path: str = None):
        """
        åˆå§‹åŒ–æäº¤æ–‡ä»¶ç”Ÿæˆå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            model_path: æ¨¡å‹æƒé‡è·¯å¾„
        """
        # åŠ è½½é…ç½®
        self.config = TrainingConfig() if config_path is None else self._load_config(config_path)
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(model_path)
        
        # éªŒè¯é›†è·¯å¾„è®¾ç½®
        self.val_data_root = "./data/val"
        self.gallery_data_root = "./data/val/gallery"  # ç”»å»Šæ•°æ®(å¯è§å…‰å›¾åƒ)
        self.query_file = "./data/val/val_queries.csv"
        
        # è¾“å‡ºè·¯å¾„
        self.output_dir = "./submissions"
        os.makedirs(self.output_dir, exist_ok=True)
        
    def _load_config(self, config_path: str):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºä»æ–‡ä»¶åŠ è½½é…ç½®
        return TrainingConfig()
    
    def _load_model(self, model_path: str = None) -> MultiModalReIDModel:
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if model_path is None:
            model_path = self.config.best_model_path
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åˆ›å»ºæ¨¡å‹
        model = MultiModalReIDModel(self.config).to(self.device)
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(model_path, map_location=self.device)
        
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
            print(f"åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
            print(f"è®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'Unknown')}")
            print(f"æœ€ä½³mAP: {checkpoint.get('best_map', 'Unknown')}")
        else:
            state_dict = checkpoint
        
        # å¤„ç†ç±»åˆ«æ•°é‡ä¸åŒ¹é…çš„é—®é¢˜
        model_state_dict = model.state_dict()
        
        # æ£€æŸ¥åˆ†ç±»å™¨æƒé‡å°ºå¯¸æ˜¯å¦åŒ¹é…
        if 'classifier.weight' in state_dict and 'classifier.weight' in model_state_dict:
            checkpoint_num_classes = state_dict['classifier.weight'].size(0)
            current_num_classes = model_state_dict['classifier.weight'].size(0)
            
            if checkpoint_num_classes != current_num_classes:
                print(f"è­¦å‘Š: æ£€æŸ¥ç‚¹ä¸­çš„ç±»åˆ«æ•°é‡ ({checkpoint_num_classes}) ä¸å½“å‰é…ç½® ({current_num_classes}) ä¸åŒ¹é…")
                print("å°†è·³è¿‡åˆ†ç±»å™¨æƒé‡çš„åŠ è½½ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å™¨")
                
                # ç§»é™¤åˆ†ç±»å™¨æƒé‡ï¼ŒåªåŠ è½½å…¶ä»–å±‚
                state_dict.pop('classifier.weight', None)
                state_dict.pop('classifier.bias', None)
        
        # åŠ è½½å…¼å®¹çš„æƒé‡
        try:
            model.load_state_dict(state_dict, strict=False)
            print("æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"è­¦å‘Š: éƒ¨åˆ†æƒé‡åŠ è½½å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨éƒ¨åˆ†åŠ è½½çš„æƒé‡ç»§ç»­")
        
        model.eval()
        return model
    
    def _load_query_list(self) -> pd.DataFrame:
        """åŠ è½½æŸ¥è¯¢åˆ—è¡¨"""
        if not os.path.exists(self.query_file):
            raise FileNotFoundError(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {self.query_file}")
        
        queries_df = pd.read_csv(self.query_file)
        print(f"åŠ è½½æŸ¥è¯¢æ•°é‡: {len(queries_df)}")
        
        # ç»Ÿè®¡å„ç±»æŸ¥è¯¢æ•°é‡
        query_types = queries_df['query_type'].value_counts()
        print("æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ:")
        for qtype, count in query_types.items():
            print(f"  {qtype}: {count}")
        
        return queries_df
    
    def _parse_query_content(self, content_str: str) -> List[str]:
        """è§£ææŸ¥è¯¢å†…å®¹å­—ç¬¦ä¸²"""
        try:
            # ä½¿ç”¨ast.literal_evalå®‰å…¨è§£æåˆ—è¡¨å­—ç¬¦ä¸²
            content_list = ast.literal_eval(content_str)
            return content_list if isinstance(content_list, list) else [content_list]
        except Exception as e:
            print(f"è§£ææŸ¥è¯¢å†…å®¹å¤±è´¥: {content_str}, é”™è¯¯: {e}")
            return []
    
    def _extract_gallery_features(self) -> Tuple[torch.Tensor, List[str]]:
        """æå–ç”»å»Šç‰¹å¾(å¯è§å…‰æ¨¡æ€)"""
        print("æå–ç”»å»Šç‰¹å¾...")
        
        gallery_path = Path(self.gallery_data_root)
        if not gallery_path.exists():
            raise FileNotFoundError(f"ç”»å»Šç›®å½•ä¸å­˜åœ¨: {gallery_path}")
        
        # æ”¶é›†æ‰€æœ‰ç”»å»Šå›¾åƒï¼ŒæŒ‰æ–‡ä»¶åæ•°å­—æ’åº
        gallery_images = []
        for img_path in gallery_path.glob("*.jpg"):
            gallery_images.append(str(img_path))
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ•°å­—æ’åºï¼ˆç¡®ä¿1.jpg, 2.jpg, ..., 10.jpgçš„æ­£ç¡®é¡ºåºï¼‰
        def extract_number(path):
            filename = Path(path).stem  # è·å–ä¸å¸¦æ‰©å±•åçš„æ–‡ä»¶å
            try:
                return int(filename)
            except ValueError:
                return float('inf')  # éæ•°å­—æ–‡ä»¶åæ’åˆ°æœ€å
                
        gallery_images.sort(key=extract_number)
        print(f"ç”»å»Šå›¾åƒæ•°é‡: {len(gallery_images)}")
        print(f"å‰5ä¸ªç”»å»Šå›¾åƒ: {[Path(p).name for p in gallery_images[:5]]}")
        
        # åˆ›å»ºç”»å»Šæ•°æ®é›†
        gallery_dataset = GalleryDataset(gallery_images, self.config)
        gallery_loader = DataLoader(
            gallery_dataset,
            batch_size=self.config.inference_batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=True,
            collate_fn=compatible_collate_fn
        )
        
        # æå–ç‰¹å¾
        gallery_features = []
        
        with torch.no_grad():
             for batch in tqdm(gallery_loader, desc="æå–ç”»å»Šç‰¹å¾"):
                 batch = self._move_to_device(batch)
                 
                 with autocast('cuda', enabled=self.device.type == 'cuda'):
                     features = self.model(batch, return_features=True)
                 
                 gallery_features.append(features.cpu())
        
        gallery_features = torch.cat(gallery_features, dim=0)
        gallery_features = F.normalize(gallery_features, p=2, dim=1)
        
        print(f"ç”»å»Šç‰¹å¾å½¢çŠ¶: {gallery_features.shape}")
        return gallery_features, gallery_images
    
    def _extract_query_features(self, queries_df: pd.DataFrame) -> Dict[str, torch.Tensor]:
        """æå–æ‰€æœ‰æŸ¥è¯¢ç‰¹å¾"""
        print("æå–æŸ¥è¯¢ç‰¹å¾...")
        
        query_features = {}
        
        # æŒ‰æŸ¥è¯¢ç±»å‹åˆ†ç»„å¤„ç†
        for query_type, group in queries_df.groupby('query_type'):
            print(f"å¤„ç†æŸ¥è¯¢ç±»å‹: {query_type} ({len(group)} ä¸ªæŸ¥è¯¢)")
            
            # åˆ›å»ºæŸ¥è¯¢æ•°æ®é›†
            query_dataset = QueryDataset(group, self.val_data_root, self.config, query_type)
            query_loader = DataLoader(
                query_dataset,
                batch_size=self.config.inference_batch_size,
                shuffle=False,
                num_workers=self.config.num_workers,
                pin_memory=True,
                collate_fn=compatible_collate_fn
            )
            
            # æå–ç‰¹å¾
            type_features = []
            
            with torch.no_grad():
                for batch in tqdm(query_loader, desc=f"æå–{query_type}ç‰¹å¾"):
                    batch = self._move_to_device(batch)
                    
                    with autocast('cuda', enabled=self.device.type == 'cuda'):
                        features = self.model(batch, return_features=True)
                    
                    type_features.append(features.cpu())
            
            if type_features:
                type_features = torch.cat(type_features, dim=0)
                type_features = F.normalize(type_features, p=2, dim=1)
                query_features[query_type] = type_features
                
                print(f"{query_type} ç‰¹å¾å½¢çŠ¶: {type_features.shape}")
        
        return query_features
    
    def _compute_similarities_and_rankings(self, 
                                         query_features: Dict[str, torch.Tensor],
                                         gallery_features: torch.Tensor,
                                         gallery_images: List[str],
                                         queries_df: pd.DataFrame,
                                         top_k: int = 100) -> List[Dict]:
        """è®¡ç®—ç›¸ä¼¼åº¦å¹¶ç”Ÿæˆæ’åºåˆ—è¡¨"""
        print("è®¡ç®—ç›¸ä¼¼åº¦å¹¶ç”Ÿæˆæ’åº...")
        
        results = []
        
        # æŒ‰æŸ¥è¯¢ç±»å‹å¤„ç†
        for query_type, group in queries_df.groupby('query_type'):
            if query_type not in query_features:
                print(f"è­¦å‘Š: æœªæ‰¾åˆ° {query_type} çš„ç‰¹å¾")
                continue
            
            qf = query_features[query_type]
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity = torch.mm(qf, gallery_features.t())  # (Q, G)
            
            # è·å–top-kæ’åº
            _, indices = torch.topk(similarity, top_k, dim=1, largest=True)
            
            # è½¬æ¢ä¸ºå®é™…çš„å›¾åƒIDï¼ˆæ–‡ä»¶åä¸­çš„æ•°å­—ï¼‰
            # ç”±äºç”»å»Šå›¾åƒæ–‡ä»¶åæ˜¯1.jpg, 2.jpg, ..., N.jpgï¼Œè€Œæ•°ç»„ç´¢å¼•æ˜¯0, 1, ..., N-1
            # æˆ‘ä»¬éœ€è¦å°†æ•°ç»„ç´¢å¼•è½¬æ¢ä¸ºå¯¹åº”çš„æ–‡ä»¶åID
            rankings = []
            for query_indices in indices:
                query_ranking = []
                for idx in query_indices:
                    # ä»æ•°ç»„ç´¢å¼•è½¬æ¢ä¸ºæ–‡ä»¶åID
                    img_path = gallery_images[idx.item()]
                    img_id = int(Path(img_path).stem)  # è·å–æ–‡ä»¶åä¸­çš„æ•°å­—
                    query_ranking.append(img_id)
                rankings.append(query_ranking)
            
            # ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆç»“æœ
            for i, (_, query_row) in enumerate(group.iterrows()):
                result = {
                    'query_idx': query_row['query_idx'],
                    'query_type': query_type,
                    'ranking_list_idx': str(rankings[i])  # è½¬ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                }
                results.append(result)
        
        print(f"ç”Ÿæˆæ’åºç»“æœ: {len(results)} ä¸ªæŸ¥è¯¢")
        return results
    
    def _move_to_device(self, batch):
        """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡"""
        if isinstance(batch, dict):
            return {k: self._move_to_device(v) for k, v in batch.items()}
        elif isinstance(batch, (list, tuple)):
            return [self._move_to_device(x) for x in batch]
        elif torch.is_tensor(batch):
            return batch.to(self.device)
        else:
            return batch
    
    def generate_submission(self, output_filename: str = "submission.csv") -> str:
        """ç”Ÿæˆæäº¤æ–‡ä»¶"""
        print("=" * 60)
        print("PRCV2025 å¤šæ¨¡æ€äººå‘˜é‡è¯†åˆ« - æäº¤æ–‡ä»¶ç”Ÿæˆ")
        print("=" * 60)
        
        # 1. åŠ è½½æŸ¥è¯¢åˆ—è¡¨
        queries_df = self._load_query_list()
        
        # 2. æå–ç”»å»Šç‰¹å¾
        gallery_features, gallery_images = self._extract_gallery_features()
        
        # 3. æå–æŸ¥è¯¢ç‰¹å¾
        query_features = self._extract_query_features(queries_df)
        
        # 4. è®¡ç®—ç›¸ä¼¼åº¦å’Œæ’åº
        results = self._compute_similarities_and_rankings(
            query_features, gallery_features, gallery_images, queries_df
        )
        
        # 5. ç”ŸæˆCSVæ–‡ä»¶
        output_path = os.path.join(self.output_dir, output_filename)
        results_df = pd.DataFrame(results)
        
        # ç¡®ä¿åˆ—é¡ºåºæ­£ç¡®
        results_df = results_df[['query_idx', 'query_type', 'ranking_list_idx']]
        
        # æŒ‰query_idxæ’åº
        results_df = results_df.sort_values('query_idx').reset_index(drop=True)
        
        # ä¿å­˜æ–‡ä»¶
        results_df.to_csv(output_path, index=False)
        
        print(f"\næäº¤æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
        print(f"æ€»æŸ¥è¯¢æ•°é‡: {len(results_df)}")
        print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
        
        # éªŒè¯æ–‡ä»¶æ ¼å¼
        self._validate_submission_format(output_path)
        
        return output_path
    
    def _validate_submission_format(self, submission_path: str):
        """éªŒè¯æäº¤æ–‡ä»¶æ ¼å¼"""
        print("\néªŒè¯æäº¤æ–‡ä»¶æ ¼å¼...")
        
        try:
            df = pd.read_csv(submission_path)
            
            # æ£€æŸ¥åˆ—å
            expected_columns = ['query_idx', 'query_type', 'ranking_list_idx']
            if list(df.columns) != expected_columns:
                print(f"è­¦å‘Š: åˆ—åä¸åŒ¹é…. æœŸæœ›: {expected_columns}, å®é™…: {list(df.columns)}")
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            print(f"query_idx èŒƒå›´: {df['query_idx'].min()} - {df['query_idx'].max()}")
            print(f"æŸ¥è¯¢ç±»å‹: {df['query_type'].nunique()} ç§")
            
            # æ£€æŸ¥ranking_list_idxæ ¼å¼
            sample_ranking = df['ranking_list_idx'].iloc[0]
            try:
                ranking_list = ast.literal_eval(sample_ranking)
                print(f"æ’åºåˆ—è¡¨é•¿åº¦: {len(ranking_list)}")
                print(f"æ’åºèŒƒå›´: {min(ranking_list)} - {max(ranking_list)}")
            except:
                print("è­¦å‘Š: ranking_list_idx æ ¼å¼å¯èƒ½æœ‰é—®é¢˜")
            
            print("âœ… æ–‡ä»¶æ ¼å¼éªŒè¯å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶æ ¼å¼éªŒè¯å¤±è´¥: {e}")


class GalleryDataset(torch.utils.data.Dataset):
    """ç”»å»Šæ•°æ®é›†(ä»…å¯è§å…‰æ¨¡æ€)"""
    
    def __init__(self, image_paths: List[str], config):
        self.image_paths = image_paths
        self.config = config
        
        # ä½¿ç”¨éªŒè¯æ—¶çš„æ•°æ®å˜æ¢
        from torchvision import transforms
        self.transform = transforms.Compose([
            transforms.Resize((config.image_size, config.image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        from PIL import Image
        
        image_path = self.image_paths[idx]
        
        try:
            image = Image.open(image_path).convert('RGB')
            image_tensor = self.transform(image)
        except Exception as e:
            print(f"åŠ è½½å›¾åƒå¤±è´¥: {image_path}, é”™è¯¯: {e}")
            image_tensor = torch.zeros(3, self.config.image_size, self.config.image_size)
        
        return {
            'person_id': torch.tensor(0, dtype=torch.long),  # å ä½ç¬¦
            'images': {'vis': image_tensor},
            'text_description': [""],
            'modality_mask': {
                'vis': 1.0, 'nir': 0.0, 'sk': 0.0, 'cp': 0.0, 'text': 0.0
            }
        }


class QueryDataset(torch.utils.data.Dataset):
    """æŸ¥è¯¢æ•°æ®é›†(æ”¯æŒå¤šæ¨¡æ€)"""
    
    def __init__(self, queries_df: pd.DataFrame, data_root: str, config, query_type: str):
        self.queries_df = queries_df.reset_index(drop=True)
        self.data_root = data_root
        self.config = config
        self.query_type = query_type
        
        # è§£ææŸ¥è¯¢å†…å®¹
        self.query_contents = []
        for _, row in self.queries_df.iterrows():
            content = self._parse_query_content(row['content'])
            self.query_contents.append(content)
        
        # æ•°æ®å˜æ¢
        from torchvision import transforms
        self.transform = transforms.Compose([
            transforms.Resize((config.image_size, config.image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def _parse_query_content(self, content_str: str) -> List[str]:
        """è§£ææŸ¥è¯¢å†…å®¹"""
        try:
            return ast.literal_eval(content_str)
        except:
            return []
    
    def __len__(self):
        return len(self.queries_df)
    
    def __getitem__(self, idx):
        from PIL import Image
        
        content = self.query_contents[idx]
        
        # åˆå§‹åŒ–æ¨¡æ€æ•°æ®
        images = {}
        modality_mask = {'vis': 0.0, 'nir': 0.0, 'sk': 0.0, 'cp': 0.0, 'text': 0.0}
        text_description = ""
        
        # å¤„ç†å›¾åƒæ¨¡æ€
        for item in content:
            if item.endswith('.jpg') or item.endswith('.png'):
                # ç¡®å®šæ¨¡æ€ç±»å‹
                if item.startswith('nir/'):
                    modality = 'nir'
                elif item.startswith('sk/'):
                    modality = 'sk'
                elif item.startswith('cp/'):
                    modality = 'cp'
                elif item.startswith('vis/'):
                    modality = 'vis'
                else:
                    continue
                
                # åŠ è½½å›¾åƒ
                image_path = os.path.join(self.data_root, item)
                try:
                    image = Image.open(image_path).convert('RGB')
                    images[modality] = self.transform(image)
                    modality_mask[modality] = 1.0
                except Exception as e:
                    print(f"åŠ è½½å›¾åƒå¤±è´¥: {image_path}, é”™è¯¯: {e}")
                    images[modality] = torch.zeros(3, self.config.image_size, self.config.image_size)
                    modality_mask[modality] = 0.0
            else:
                # æ–‡æœ¬æè¿°
                text_description = item
                modality_mask['text'] = 1.0
        
        # å¡«å……ç¼ºå¤±çš„æ¨¡æ€
        for modality in ['vis', 'nir', 'sk', 'cp']:
            if modality not in images:
                images[modality] = torch.zeros(3, self.config.image_size, self.config.image_size)
        
        return {
            'person_id': torch.tensor(0, dtype=torch.long),  # å ä½ç¬¦
            'images': images,
            'text_description': [text_description],
            'modality_mask': modality_mask
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='PRCV2025 æäº¤æ–‡ä»¶ç”Ÿæˆå™¨')
    parser.add_argument('--model_path', type=str, default=None,
                       help='æ¨¡å‹æƒé‡è·¯å¾„ (é»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„best_model_path)')
    parser.add_argument('--output', type=str, default='submission.csv',
                       help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--config_path', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = PRCV2025SubmissionGenerator(
        config_path=args.config_path,
        model_path=args.model_path
    )
    
    # ç”Ÿæˆæäº¤æ–‡ä»¶
    output_path = generator.generate_submission(args.output)
    
    print(f"\nğŸ‰ æäº¤æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {output_path}")
    print("ç°åœ¨å¯ä»¥å°†æ­¤æ–‡ä»¶æäº¤åˆ°æ¯”èµ›å¹³å°!")


if __name__ == "__main__":
    main()