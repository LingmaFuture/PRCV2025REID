# train.py
import os
import time
import math
import random
import pickle
import logging
from collections import defaultdict
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, MultiStepLR, LambdaLR, ReduceLROnPlateau
from torch.amp import autocast, GradScaler

# ç«‹åˆ»æ­¢è¡€ï¼šå¼€å¯ TF32 åŠ é€Ÿ
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
from sklearn.model_selection import train_test_split
import torchvision.transforms as transforms

# === ä½ é¡¹ç›®é‡Œçš„æ•°æ®ä¸æ¨¡å‹ ===
from datasets.dataset import MultiModalDataset, BalancedBatchSampler, compatible_collate_fn
from models.model import CLIPBasedMultiModalReIDModel  
from models.sdm_scheduler import SDMScheduler
from configs.config import TrainingConfig

# ------------------------------
# å®ç”¨å·¥å…·
# ------------------------------
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True
    
    # åŠ é€Ÿä¼˜åŒ–ï¼šå¯ç”¨TF32å’Œä¼˜åŒ–CUDAæ“ä½œ
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

def setup_logging(log_dir):
    os.makedirs(log_dir, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(log_dir, "training.log")),
            logging.StreamHandler(),
        ],
    )

def move_batch_to_device(batch, device):
    """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆä»…ç§»åŠ¨ Tensorï¼‰"""
    if isinstance(batch, dict):
        out = {}
        for k, v in batch.items():
            out[k] = move_batch_to_device(v, device)
        return out
    elif isinstance(batch, list):
        return [move_batch_to_device(x, device) for x in batch]
    elif isinstance(batch, tuple):
        return tuple(move_batch_to_device(x, device) for x in batch)
    elif torch.is_tensor(batch):
        return batch.to(device)
    else:
        return batch

def _sanitize_grads(model):
    """å°†éæœ‰é™æ¢¯åº¦ç½®é›¶ï¼Œé¿å… GradNorm=nan/inf ä¼ æ’­"""
    any_bad = False
    for p in model.parameters():
        if p.grad is None:
            continue
        g = p.grad
        bad = ~torch.isfinite(g)
        if bad.any():
            g[bad] = 0.0
            any_bad = True
    return any_bad

# ------------------------------
# è¯„ä¼°æŒ‡æ ‡
# ------------------------------
def compute_map(query_features, gallery_features, query_labels, gallery_labels, k=100):
    """mAP@k"""
    # ç¡®ä¿ä¸¤ä¸ªç‰¹å¾å¼ é‡å…·æœ‰ç›¸åŒçš„æ•°æ®ç±»å‹ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºfloat32
    query_features = query_features.float()
    gallery_features = gallery_features.float()
    
    query_features = F.normalize(query_features, p=2, dim=1)
    gallery_features = F.normalize(gallery_features, p=2, dim=1)
    sim = torch.mm(query_features, gallery_features.t())  # (Q, G)

    aps = []
    for i in range(sim.size(0)):
        scores = sim[i]
        qy = query_labels[i]
        _, idx = torch.sort(scores, descending=True)
        ranked = gallery_labels[idx[:k]]

        matches = (ranked == qy).float()
        if matches.sum() > 0:
            cum_matches = torch.cumsum(matches, dim=0)
            ranks = torch.arange(1, matches.numel() + 1, device=matches.device, dtype=matches.dtype)
            precision = cum_matches / ranks
            ap = precision[matches.bool()].mean().item()
            aps.append(ap)

    return float(np.mean(aps)) if aps else 0.0

def compute_cmc(query_features, gallery_features, query_labels, gallery_labels, k=10):
    """CMC@k"""
    query_features = F.normalize(query_features, p=2, dim=1)
    gallery_features = F.normalize(gallery_features, p=2, dim=1)
    sim = torch.mm(query_features, gallery_features.t())
    correct = 0
    for i in range(sim.size(0)):
        _, idx = torch.sort(sim[i], descending=True)
        topk_labels = gallery_labels[idx[:k]]
        correct += (topk_labels == query_labels[i]).any().item()
    return correct / sim.size(0) if sim.size(0) > 0 else 0.0

# ------------------------------
# åˆ’åˆ†
# ------------------------------
def split_train_dataset(dataset, val_ratio=0.2, seed=42):
    person_ids = [dataset.data_list[i]['person_id'] for i in range(len(dataset))]
    person_ids = sorted(list(set(person_ids)))
    train_ids, val_ids = train_test_split(person_ids, test_size=val_ratio, random_state=seed)

    train_indices, val_indices = [], []
    for i, sample in enumerate(dataset.data_list):
        if sample['person_id'] in train_ids:
            train_indices.append(i)
        else:
            val_indices.append(i)


    return train_indices, val_indices, train_ids, val_ids

# ------------------------------
# èµ›åˆ¶å¯¹é½æ•°æ®æ„å»º
# ------------------------------
MODALITIES = ['vis', 'nir', 'sk', 'cp', 'text']

# æ¨¡æ€åç§°æ˜ å°„ï¼šå…¼å®¹æ–°CLIP+MERæ¶æ„
MODALITY_MAPPING = {
    'vis': 'rgb',      # å¯è§å…‰ -> RGB
    'nir': 'ir',       # è¿‘çº¢å¤– -> IR
    'sk': 'sketch',    # ç´ æ -> sketch
    'cp': 'cpencil',   # å½©é“… -> cpencil
    'text': 'text'     # æ–‡æœ¬ä¿æŒä¸å˜
}

def map_modality_name(old_name: str) -> str:
    """å°†æ•°æ®é›†çš„æ¨¡æ€åç§°æ˜ å°„åˆ°æ–°æ¶æ„çš„æ¨¡æ€åç§°"""
    return MODALITY_MAPPING.get(old_name, old_name)


def move_batch_to_device(batch, device):
    """å°†batchæ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    def _move_to_device(obj):
        if torch.is_tensor(obj):
            return obj.to(device)
        elif isinstance(obj, dict):
            return {k: _move_to_device(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [_move_to_device(item) for item in obj]
        else:
            return obj
    
    return _move_to_device(batch)


def convert_batch_for_clip_model(batch):
    """
    å°†æ•°æ®é›†çš„batchæ ¼å¼è½¬æ¢ä¸ºCLIP+MERæ¨¡å‹çš„è¾“å…¥æ ¼å¼
    ä¿®å¤ï¼šæŒ‰modality_maskè¿‡æ»¤æœ‰æ•ˆæ¨¡æ€ï¼Œé¿å…é›¶ç‰¹å¾æ±¡æŸ“èåˆ
    Args:
        batch: æ•°æ®é›†è¿”å›çš„batchï¼Œæ ¼å¼ä¸º {'images': {...}, 'text_description': [...], 'modality_mask': {...}}
    Returns:
        images: {modality: tensor} å›¾åƒå­—å…¸ï¼Œä»…åŒ…å«æœ‰æ•ˆæ¨¡æ€ï¼Œæ¨¡æ€åç§°å·²æ˜ å°„
        texts: List[str] æ–‡æœ¬åˆ—è¡¨ï¼Œä»…åŒ…å«æœ‰æ•ˆæ–‡æœ¬
    """
    images = {}
    texts = None
    
    # è·å–æ¨¡æ€æ©ç 
    modality_mask = batch.get('modality_mask', {})
    
    # å¤„ç†å›¾åƒæ•°æ® - å…³é”®ä¿®å¤ï¼šåªå¤„ç†maskæŒ‡ç¤ºä¸ºæœ‰æ•ˆçš„æ¨¡æ€
    if 'images' in batch:
        for old_modality, image_tensor in batch['images'].items():
            if torch.is_tensor(image_tensor) and image_tensor.numel() > 0:
                # æ£€æŸ¥è¯¥æ¨¡æ€æ˜¯å¦åœ¨æ•´ä¸ªbatchä¸­æœ‰ä»»ä½•æœ‰æ•ˆæ ·æœ¬
                mask_tensor = modality_mask.get(old_modality, torch.zeros(image_tensor.size(0)))
                if isinstance(mask_tensor, torch.Tensor) and mask_tensor.sum() > 0:
                    # è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬åœ¨è¯¥æ¨¡æ€ä¸‹æœ‰æ•ˆï¼Œæ‰åŒ…å«è¯¥æ¨¡æ€
                    new_modality = map_modality_name(old_modality)
                    images[new_modality] = image_tensor
    
    # å¤„ç†æ–‡æœ¬æ•°æ® - ä¿®å¤ï¼šå§‹ç»ˆä¿æŒbatch sizeä¸€è‡´ï¼Œç”¨ç©ºå­—ç¬¦ä¸²å ä½
    if 'text_description' in batch:
        text_descriptions = batch['text_description']
        if isinstance(text_descriptions, list) and len(text_descriptions) > 0:
            # è·å–æ–‡æœ¬mask
            text_mask = modality_mask.get('text', torch.ones(len(text_descriptions)))
            if isinstance(text_mask, torch.Tensor):
                # å§‹ç»ˆä¿æŒbatch sizeï¼Œç”¨ç©ºå­—ç¬¦ä¸²å¡«å……æ— æ•ˆæ–‡æœ¬ä½ç½®
                processed_texts = []
                has_any_valid = False
                for i, (text, mask_val) in enumerate(zip(text_descriptions, text_mask)):
                    if mask_val > 0.5 and text and text.strip():
                        processed_texts.append(text.strip())
                        has_any_valid = True
                    else:
                        processed_texts.append("")  # ç©ºæ–‡æœ¬å ä½ï¼Œè®©CLIPè‡ªç„¶å¤„ç†
                
                # åªè¦batchä¸­æœ‰æ–‡æœ¬å­—æ®µï¼Œå°±ä¼ é€’ç»™æ¨¡å‹ï¼ˆåŒ…å«ç©ºå­—ç¬¦ä¸²ï¼‰
                # CLIP tokenizerå¯ä»¥æ­£å¸¸å¤„ç†ç©ºå­—ç¬¦ä¸²
                texts = processed_texts
    
    return images, texts


def call_model_with_batch(model, batch, return_features=False):
    """
    ä½¿ç”¨batchæ•°æ®è°ƒç”¨CLIP+MERæ¨¡å‹
    Args:
        model: CLIP+MERæ¨¡å‹
        batch: æ•°æ®é›†batch
        return_features: æ˜¯å¦è¿”å›ç‰¹å¾
    Returns:
        æ¨¡å‹è¾“å‡º
    """
    images, texts = convert_batch_for_clip_model(batch)
    
    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ç§æ¨¡æ€çš„è¾“å…¥
    if not images and not texts:
        raise ValueError("Batchä¸­æ²¡æœ‰æœ‰æ•ˆçš„å›¾åƒæˆ–æ–‡æœ¬æ•°æ®")
    
    # è·å–modality_masks
    modality_masks = batch.get('modality_mask', None)
    
    # è°ƒç”¨æ¨¡å‹ï¼ˆä¼ é€’maskä¿¡æ¯ï¼‰
    return model(images=images if images else None, 
                texts=texts if texts else None, 
                modality_masks=modality_masks,
                return_features=return_features)

def build_val_presence_table(dataset, val_indices):
    presence = {}
    for idx in val_indices:
        entry = dataset.data_list[idx]
        pid_str = entry['person_id_str']
        has = {m: False for m in ['vis','nir','sk','cp','text']}
        cache = dataset.image_cache.get(pid_str, {})
        for m in ['vis','nir','sk','cp']:
            has[m] = len(cache.get(m, [])) > 0
        td = entry.get('text_description', '')
        has['text'] = isinstance(td, str) and len(td) > 0
        presence[idx] = has
    return presence

class GalleryOnlyVIS(Dataset):
    """ç”»å»Šåªä¿ç•™å¯è§å…‰ï¼ˆvisï¼‰çš„ wrapper"""
    def __init__(self, base_dataset: Dataset, indices: List[int], presence: Dict[int, Dict[str, bool]]):
        self.base = base_dataset
        self.indices = [i for i in indices if presence.get(i, {}).get('vis', False)]

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, i):
        base_idx = self.indices[i]
        rec = self.base[base_idx]
        images = {}
        if 'images' in rec and 'vis' in rec['images']:
            images['vis'] = rec['images']['vis']
        pid = rec['person_id']
        if torch.is_tensor(pid):
            pid = int(pid.item())
        return {
            'person_id': torch.tensor(pid, dtype=torch.long),
            'images': images,
            'text_description': [""],
            'modality_mask': {'vis': True, 'nir': False, 'sk': False, 'cp': False, 'text': False}
        }

class CombinationQueryDataset(Dataset):
    """
    æŸ¥è¯¢é›†åˆï¼šå¯¹æ¯ä¸ªèº«ä»½ indexï¼Œé€‰æ‹©æŒ‡å®šçš„æ¨¡æ€ç»„åˆï¼ˆä¸å« visï¼‰
    """
    def __init__(self, base_dataset: Dataset, q_items: List[Dict]):
        self.base = base_dataset
        self.items = q_items

    def __len__(self):
        return len(self.items)

    def __getitem__(self, i):
        info = self.items[i]
        base_idx = info['idx']
        mods = info['modalities']
        rec = self.base[base_idx]

        images = {}
        if 'images' in rec and isinstance(rec['images'], dict):
            for m in mods:
                if m in ['nir', 'sk', 'cp'] and m in rec['images'] and torch.is_tensor(rec['images'][m]):
                    images[m] = rec['images'][m]

        text_desc = ""
        if 'text' in mods:
            td = rec.get('text_description', "")
            if isinstance(td, list):
                text_desc = td[0] if len(td) > 0 else ""
            elif isinstance(td, str):
                text_desc = td

        pid_t = rec['person_id']
        pid = int(pid_t.item()) if torch.is_tensor(pid_t) else int(pid_t)

        modality_mask = {'vis': False, 'nir': False, 'sk': False, 'cp': False, 'text': False}
        for m in mods:
            modality_mask[m] = True
            
        return {
            'person_id': torch.tensor(pid, dtype=torch.long),
            'images': images,
            'text_description': [text_desc],
            'modality_mask': modality_mask
        }

def build_eval_loaders_by_rule(dataset, val_indices, batch_size, num_workers, pin_memory):
    presence = build_val_presence_table(dataset, val_indices)
    gal_ds = GalleryOnlyVIS(dataset, val_indices, presence)
    gallery_loader = DataLoader(
        gal_ds,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True,
        prefetch_factor=2,
        collate_fn=compatible_collate_fn
    )
    non_vis = ['nir', 'sk', 'cp', 'text']

    def _make_items(group: List[str]) -> List[Dict]:
        items = []
        for idx in val_indices:
            has = presence[idx]
            if all(has.get(m, False) for m in group):
                items.append({'idx': idx,  'modalities': group})
        return items

    query_loaders = {'single': {}, 'double': {}, 'triple': {}, 'quad': {}}

    # å•æ¨¡æ€
    for m in non_vis:
        items = _make_items([m])
        if items:
            loader = DataLoader(
                CombinationQueryDataset(dataset, items),
                batch_size=batch_size,
                shuffle=False,
                num_workers=num_workers,
                pin_memory=pin_memory,
                persistent_workers=True,
                prefetch_factor=2,
                collate_fn=compatible_collate_fn
            )
            query_loaders['single'][m] = loader

    # åŒ/ä¸‰/å››æ¨¡æ€
    import itertools
    for gsize, tag in [(2, 'double'), (3, 'triple'), (4, 'quad')]:
        for comb in itertools.combinations(non_vis, gsize):
            group = list(comb)
            key = '+'.join(group)
            items = _make_items(group)
            if items:
                loader = DataLoader(
                    CombinationQueryDataset(dataset, items),
                    batch_size=batch_size,
                    shuffle=False,
                    num_workers=num_workers,
                    pin_memory=pin_memory,
                    persistent_workers=True,
                    prefetch_factor=2,
                    collate_fn=compatible_collate_fn
                )
                query_loaders[tag][key] = loader

    return gallery_loader, query_loaders


def _subsample_features(feats: torch.Tensor, labels: torch.Tensor, ratio: float, rng: Optional[torch.Generator]=None):
    """åœ¨æ‹¼æ¥åæŒ‰æ ·æœ¬ç»´åº¦è¿›è¡Œå­é‡‡æ ·ï¼Œé¿å…æŒ‰ batch é‡‡æ ·çš„åç½®"""
    if ratio >= 0.999 or feats.size(0) <= 1:
        return feats, labels
    n = max(1, int(math.ceil(feats.size(0) * ratio)))
    perm = torch.randperm(feats.size(0), generator=rng, device=feats.device)[:n]
    return feats[perm], labels[perm]

def validate_competition_style(model, gallery_loader, query_loaders, device, k_map=100, sample_ratio=1.0):
    """
    èµ›åˆ¶å¯¹é½è¯„æµ‹ï¼šmAP/CMCï¼›æŒ‰æ ·æœ¬å­é›†é‡‡æ ·é¿å…åç½®
    
    å…³é”®ä¸€è‡´æ€§åŸåˆ™ï¼š
    1. å¼ºåˆ¶ä½¿ç”¨bn_featuresè¿›è¡Œæ£€ç´¢ï¼Œä¸è®­ç»ƒä¸­çš„å¯¹é½æŸå¤±ä¿æŒå®Œå…¨ä¸€è‡´
    2. èåˆé˜¶æ®µå·²é€šè¿‡maskå¤„ç†ç¼ºå¤±æ¨¡æ€ï¼Œæ£€ç´¢æ—¶æ— éœ€é¢å¤–maskå¤„ç†
    3. ç›´æ¥L2å½’ä¸€åŒ–+ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼Œç®€æ´é«˜æ•ˆ
    """
    model.eval()
    rng = torch.Generator(device=device)
    rng.manual_seed(1234)

    with torch.no_grad():
        gal_feats, gal_labels = [], []
        for batch in tqdm(gallery_loader, desc=f'æå–ç”»å»Šç‰¹å¾(å…¨é‡)', 
                          leave=False, ncols=100, mininterval=0.5):
            batch = move_batch_to_device(batch, device)
            with autocast(device_type='cuda', dtype=torch.float16, enabled=device.type == 'cuda'):
                outputs = call_model_with_batch(model, batch, return_features=True)
                # å¼ºåˆ¶ä½¿ç”¨BNåç‰¹å¾è¿›è¡Œæ£€ç´¢ï¼Œç¡®ä¿ä¸è®­ç»ƒå¯¹é½æŸå¤±ä¸€è‡´
                if 'bn_features' in outputs:
                    feats = outputs['bn_features']  # BNåç‰¹å¾ï¼Œä¸å¯¹é½æŸå¤±ä¸€è‡´
                else:
                    raise ValueError("æ¨¡å‹è¾“å‡ºç¼ºå°‘bn_featuresï¼Œæ£€ç´¢ç‰¹å¾ä¸ä¸€è‡´")
            labels = batch['person_id']
            gal_feats.append(feats.cpu()); gal_labels.append(labels.cpu())
        gal_feats = torch.cat(gal_feats, dim=0)
        gal_labels = torch.cat(gal_labels, dim=0)

    # å¯¹ç”»å»Šè¿›è¡Œæ ·æœ¬çº§é‡‡æ ·ï¼ˆä»…ç”¨äºç›¸ä¼¼åº¦è®¡ç®—é˜¶æ®µï¼‰
    if sample_ratio < 1.0 and gal_feats.size(0) > 1:
        idx = torch.randperm(gal_feats.size(0))[:max(1, int(gal_feats.size(0)*sample_ratio))]
        gal_feats = gal_feats[idx]; gal_labels = gal_labels[idx]

    # å¿«é€ŸéªŒè¯ï¼šåªæµ‹singleå’Œquadæ¨¡æ€ï¼Œè·³è¿‡doubleå’Œtriple
    detail = {'single': {}, 'quad': {}}
    buckets = {'single': [], 'quad': []}
    all_q_feats, all_q_labels = [], []

    with torch.no_grad():
        for tag, group in query_loaders.items():
            # åªå¤„ç†å•æ¨¡æ€å’Œå››æ¨¡æ€æŸ¥è¯¢ï¼Œè·³è¿‡åŒæ¨¡æ€å’Œä¸‰æ¨¡æ€
            if tag not in ['single', 'quad']:
                continue
                
            for key, qloader in group.items():
                qf, ql = [], []
                for batch in tqdm(qloader, desc=f'æå–æŸ¥è¯¢ç‰¹å¾[{tag}:{key}]', 
                                  leave=False, ncols=100, mininterval=0.5):
                    batch = move_batch_to_device(batch, device)
                    with autocast(device_type='cuda', dtype=torch.float16, enabled=device.type == 'cuda'):
                        outputs = call_model_with_batch(model, batch, return_features=True)
                        # å¼ºåˆ¶ä½¿ç”¨BNåç‰¹å¾è¿›è¡Œæ£€ç´¢ï¼Œç¡®ä¿ä¸è®­ç»ƒå¯¹é½æŸå¤±ä¸€è‡´
                        if 'bn_features' in outputs:
                            feats = outputs['bn_features']  # BNåç‰¹å¾ï¼Œä¸å¯¹é½æŸå¤±ä¸€è‡´
                        else:
                            raise ValueError("æ¨¡å‹è¾“å‡ºç¼ºå°‘bn_featuresï¼Œæ£€ç´¢ç‰¹å¾ä¸ä¸€è‡´")
                    labels = batch['person_id']
                    qf.append(feats.cpu()); ql.append(labels.cpu())
                if not qf:
                    continue
                qf = torch.cat(qf, dim=0); ql = torch.cat(ql, dim=0)

                # å¯¹æŸ¥è¯¢ä¹Ÿåšæ ·æœ¬çº§é‡‡æ ·
                qf, ql = _subsample_features(qf, ql, sample_ratio)

                km = min(k_map, gal_feats.size(0))
                m = compute_map(qf, gal_feats, ql, gal_labels, k=km)

                detail[tag][key] = float(m)
                buckets[tag].append(m)
                all_q_feats.append(qf); all_q_labels.append(ql)

    def _avg(x): return float(np.mean(x)) if x else 0.0
    map_single = _avg(buckets['single'])
    map_quad   = _avg(buckets['quad'])
    # å¿«é€Ÿè¯„ä¼°ï¼šåªç”¨singleå’Œquadçš„å¹³å‡
    map_avg2   = float(np.mean([map_single, map_quad])) if (map_single > 0 or map_quad > 0) else 0.0

    if all_q_feats:
        all_q_feats = torch.cat(all_q_feats, dim=0)
        all_q_labels = torch.cat(all_q_labels, dim=0)
        cmc1 = compute_cmc(all_q_feats, gal_feats, all_q_labels, gal_labels, k=1)
        cmc5 = compute_cmc(all_q_feats, gal_feats, all_q_labels, gal_labels, k=5)
        cmc10 = compute_cmc(all_q_feats, gal_feats, all_q_labels, gal_labels, k=10)
    else:
        cmc1 = cmc5 = cmc10 = 0.0

    return {
        'map_single': map_single,
        'map_quad': map_quad,
        'map_avg2': map_avg2,  # singleå’Œquadçš„å¹³å‡
        'detail': detail,
        'cmc1': cmc1, 'cmc5': cmc5, 'cmc10': cmc10
    }

# ------------------------------
# è®­ç»ƒä¸€ä¸ª epoch
# ------------------------------
def train_epoch(model, dataloader, optimizer, device, epoch, scaler=None, adaptive_clip=True, accum_steps=1, autocast_dtype=torch.float16):
    model.train()
    
    # è®¾ç½®å½“å‰epochï¼Œç”¨äºæ§åˆ¶modality_dropoutçƒ­èº«æœŸ
    model.set_epoch(epoch)
    total_loss = 0.0
    ce_loss_sum = 0.0
    contrastive_loss_sum = 0.0
    correct = 0
    total = 0
    
    feature_norms = []
    loss_spikes = 0
    grad_norms = []
    
    # ç¨³å¥çš„Spikeæ£€æµ‹çŠ¶æ€ç®¡ç†
    if not hasattr(train_epoch, '_spike_state'):
        train_epoch._spike_state = {
            'loss_hist': [],
            'spikes': 0,
            'batches': 0
        }
    
    use_amp = (scaler is not None and getattr(scaler, "is_enabled", lambda: True)())

    pbar = tqdm(dataloader, desc=f'Epoch {epoch}', 
                leave=True, ncols=120, mininterval=2.0, maxinterval=5.0)
    
    # æ·»åŠ æ‰¹æ¬¡æ„æˆç›‘æ§ï¼ˆå‰3ä¸ªbatchï¼‰
    if epoch <= 3:
        logging.info(f"=== Epoch {epoch} æ‰¹æ¬¡æ„æˆç›‘æ§ ===")
    
    for batch_idx, batch in enumerate(pbar):
        batch = move_batch_to_device(batch, device)
        labels = batch['person_id']
        
        # æ‰¹æ¬¡æ„æˆç›‘æ§ï¼ˆå‰3ä¸ªepochçš„å‰3ä¸ªbatchï¼‰
        if epoch <= 3 and batch_idx < 3:
            unique_ids, counts = torch.unique(labels, return_counts=True)
            num_ids_per_batch = len(unique_ids)
            avg_instances_per_id = float(counts.float().mean().item())  # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å†è®¡ç®—å‡å€¼
            logging.info(f"Batch {batch_idx}: {num_ids_per_batch} IDs, å¹³å‡æ¯ID {avg_instances_per_id:.1f} æ ·æœ¬ (K-1æ­£æ ·æœ¬æ•°â‰ˆ{avg_instances_per_id-1:.1f})")

        # æ¢¯åº¦ç´¯ç§¯ï¼šåªåœ¨ç´¯ç§¯æ­¥å¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
        if batch_idx % accum_steps == 0:
            optimizer.zero_grad(set_to_none=True)
        # === SDMæƒé‡è°ƒåº¦ï¼ˆåªè¯»å½“å‰å‚æ•°ï¼Œé¿å…ç©ºæŒ‡æ ‡è§¦å‘å›é€€ï¼‰ ===
        if not hasattr(model, 'sdm_scheduler'):
            model.sdm_scheduler = SDMScheduler(model.config)
        
        # âœ… åªè¯»å½“å‰æƒé‡/æ¸©åº¦ï¼Œä¸åšåˆ¤å®šï¼ˆç”± epoch ç»“æŸåç»Ÿä¸€è°ƒæ•´ï¼‰
        effective_cont_w = getattr(model.sdm_scheduler.weight_scheduler, "current_weight",
                                   getattr(model.config, "contrastive_weight", 0.1))
        current_temp = getattr(model.sdm_scheduler.temp_scheduler, "current_temp",
                               getattr(model.config, "sdm_temperature", 0.2))
        
        # æ¯ä¸ªepochçš„ç¬¬ä¸€ä¸ªbatchè¾“å‡ºSDMå‚æ•°ï¼ˆåªè¯»æ¨¡å¼ï¼‰
        if batch_idx == 0:
            logging.info(f"Epoch {epoch}: (read-only) SDM_weight={effective_cont_w:.4f}, SDM_temp={current_temp:.3f}")

        with autocast(device_type='cuda', dtype=autocast_dtype, enabled=use_amp):
            try:
                outputs = call_model_with_batch(model, batch, return_features=False)
                loss_dict = model.compute_loss(outputs, labels)
            except RuntimeError as e:
                if "out of memory" in str(e):
                    # å†…å­˜ä¸è¶³æ—¶æ¸…ç†ç¼“å­˜å¹¶è·³è¿‡
                    torch.cuda.empty_cache()
                    logging.warning(f"Epoch {epoch}, Batch {batch_idx}: å†…å­˜ä¸è¶³ï¼Œè·³è¿‡å½“å‰batch")
                    continue
                else:
                    raise e
            
            # NaNæ£€æµ‹å’Œè·³è¿‡æœºåˆ¶ï¼ˆè§£å†³ç¬¬29batché—®é¢˜ï¼‰
            total_loss_val = loss_dict['total_loss']
            if not torch.isfinite(total_loss_val):
                logging.warning(f"Epoch {epoch}, Batch {batch_idx}: å‘ç°éæœ‰é™æŸå¤±å€¼ {total_loss_val.item():.6f}ï¼Œè·³è¿‡å½“å‰step")
                optimizer.zero_grad(set_to_none=True)
                continue
            
            # SDMå¯¹æ¯”æŸå¤±ç›‘æ§ï¼ˆä»…åœ¨å®é™…å‚ä¸è®­ç»ƒæ—¶è­¦å‘Šï¼‰
            sdm_loss = loss_dict.get('sdm_loss', torch.tensor(0.0, device=device))
            cont_loss = loss_dict.get('contrastive_loss', sdm_loss)  # å…¼å®¹æ€§ï¼šç”¨sdm_lossä½œä¸ºcontrastive_loss
            
            # åªæœ‰å½“SDMæŸå¤±çœŸæ­£å‚ä¸è®­ç»ƒæ—¶ï¼ˆæƒé‡>0ï¼‰æ‰æ˜¾ç¤ºè­¦å‘Š
            if effective_cont_w > 0.0 and sdm_loss.item() > 1.5:  
                original_sdm = sdm_loss.item()
                if batch_idx < 5:  # åªåœ¨å‰å‡ ä¸ªbatchæŠ¥å‘Šï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    logging.info(f"Epoch {epoch}, Batch {batch_idx}: SDMå¯¹é½æŸå¤±è¾ƒé«˜ {original_sdm:.3f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
            if not torch.isfinite(sdm_loss):
                logging.error(f"Epoch {epoch}, Batch {batch_idx}: SDMæŸå¤±å‡ºç°NaN/Inf, é‡ç½®ä¸º0")
                loss_dict['sdm_loss'] = torch.tensor(0.0, device=device)
                loss_dict['contrastive_loss'] = torch.tensor(0.0, device=device)
                # é‡æ–°è®¡ç®—æ€»æŸå¤±
                ce_loss = loss_dict.get('ce_loss', torch.tensor(0.0, device=device))
                feat_penalty = loss_dict.get('feat_penalty', torch.tensor(0.0, device=device))
                sdm_weight = getattr(model.config, 'contrastive_weight', 0.1)
                loss_dict['total_loss'] = ce_loss + sdm_weight * loss_dict['sdm_loss'] + feat_penalty
            
            loss = loss_dict['total_loss']
        
        # ç”¨ warmup åçš„æœ‰æ•ˆæƒé‡é‡ç®—æ€»æŸå¤±ï¼ˆåŒ…å«ç‰¹å¾èŒƒæ•°æ­£åˆ™é¡¹ï¼‰
        ce_loss = loss_dict.get('ce_loss', torch.tensor(0.0, device=device))
        sdm_loss = loss_dict.get('sdm_loss', torch.tensor(0.0, device=device))
        cont_loss = loss_dict.get('contrastive_loss', sdm_loss)  # å…¼å®¹æ€§
        feat_penalty = loss_dict.get('feat_penalty', torch.tensor(0.0, device=device))
        loss = ce_loss + effective_cont_w * sdm_loss + feat_penalty  # ä½¿ç”¨SDMæŸå¤±
        loss = loss / accum_steps  # æ¢¯åº¦ç´¯ç§¯ï¼šç¼©æ”¾æŸå¤±
        loss_dict['total_loss'] = loss

        current_loss = float(loss.item() * accum_steps)  # æ˜¾ç¤ºæœªç¼©æ”¾çš„æŸå¤±
        
        # ç¨³å¥çš„Spikeæ£€æµ‹ï¼šä½¿ç”¨æ»‘åŠ¨ä¸­ä½æ•° + MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰
        state = train_epoch._spike_state
        state['loss_hist'].append(current_loss)
        # ä¿æŒæœ€è¿‘200ä¸ªæŸå¤±å€¼çš„å†å²
        if len(state['loss_hist']) > 200:
            state['loss_hist'] = state['loss_hist'][-200:]
        
        # âœ… æ¡ä»¶å¯åŠ¨ï¼šè¶³å¤Ÿæ ·æœ¬å†å¼€å¯spikeæ£€æµ‹
        if len(state['loss_hist']) >= 20:
            hist = np.array(state['loss_hist'][-100:])         # âœ… æœ€è¿‘100ä¸ªæ ·æœ¬
            median = np.median(hist)
            mad = np.median(np.abs(hist - median))
            mad = max(mad, 0.05)                                # âœ… MADä¸‹é™
            threshold = max(median + 6.0 * 1.4826 * mad,       # âœ… ç»å¯¹é—¨é™
                            median * 1.15)                     # âœ… ç›¸å¯¹é—¨æ§› 15%
            
            # æ£€æµ‹å¼‚å¸¸
            if current_loss > threshold:
                loss_spikes += 1
                state['spikes'] += 1
                if batch_idx % 20 == 0:  # å‡å°‘æ—¥å¿—é¢‘ç‡
                    logging.warning(f"Epoch {epoch}, Batch {batch_idx}: æŸå¤±å¼‚å¸¸ {current_loss:.3f} > {threshold:.3f}")
        
        state['batches'] += 1
        
        # æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
        if not np.isfinite(current_loss):
            logging.error(f"Epoch {epoch}, Batch {batch_idx}: æŸå¤±æ— æ•ˆ {current_loss}, è·³è¿‡")
            loss_spikes += 1
            state['spikes'] += 1
            continue
        
        # è®¡ç®—æ¢¯åº¦ï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
        if use_amp and scaler is not None:
            scaler.scale(loss).backward()
        else:
            loss.backward()
            
        # åªåœ¨ç´¯ç§¯æ­¥ç»“æŸæ—¶æ›´æ–°å‚æ•°
        if (batch_idx + 1) % accum_steps == 0:
            if use_amp and scaler is not None:
                scaler.unscale_(optimizer)
                _sanitize_grads(model)

                total_norm = 0.0
                for p in model.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.detach().data.norm(2)
                        if torch.isfinite(param_norm):
                            total_norm += param_norm.item() ** 2
                total_norm = math.sqrt(total_norm)
                grad_norms.append(float(total_norm))

                # è‡ªé€‚åº”æ¢¯åº¦è£å‰ªï¼ˆæ”¶ç´§ä¸Šé™æå‡ç¨³å®šæ€§ï¼‰
                if adaptive_clip:
                    if len(grad_norms) > 10:
                        recent_norms = grad_norms[-10:]
                        adaptive_max_norm = min(3.0, max(0.5, np.percentile(recent_norms, 70) * 1.15))
                    else:
                        adaptive_max_norm = 1.0
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(adaptive_max_norm))
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # é™ä½æ¢¯åº¦è£å‰ªé˜ˆå€¼ä¿®å¤CEæ”¶æ•›

                scaler.step(optimizer)
                scaler.update()
            else:
                _sanitize_grads(model)

                total_norm = 0.0
                for p in model.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.detach().data.norm(2)
                        if torch.isfinite(param_norm):
                            total_norm += param_norm.item() ** 2
                total_norm = math.sqrt(total_norm)
                grad_norms.append(float(total_norm))

                if adaptive_clip:
                    if len(grad_norms) > 10:
                        recent_norms = grad_norms[-10:]
                        adaptive_max_norm = min(3.0, max(0.5, np.percentile(recent_norms, 70) * 1.15))
                    else:
                        adaptive_max_norm = 1.0
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(adaptive_max_norm))
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # é™ä½æ¢¯åº¦è£å‰ªé˜ˆå€¼ä¿®å¤CEæ”¶æ•›

                optimizer.step()
                
                # å®šæœŸæ¸…ç†CUDAç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜ç´¯ç§¯
                if (batch_idx + 1) % (accum_steps * 5) == 0:  # æ¯5ä¸ªç´¯ç§¯å‘¨æœŸæ¸…ç†ä¸€æ¬¡
                    torch.cuda.empty_cache()

        # ç»Ÿè®¡ä¿¡æ¯æ›´æ–°ï¼ˆç§»å›å¾ªç¯å†…éƒ¨ï¼‰
        total_loss += current_loss
        ce_loss_sum += float(ce_loss.item())
        contrastive_loss_sum += float(sdm_loss.item())  # ç´¯è®¡SDMæŸå¤±

        if isinstance(outputs, dict) and 'logits' in outputs:
            _, predicted = outputs['logits'].max(1)
        else:
            _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # ç›‘æ§ä¸‰ç§ç‰¹å¾èŒƒæ•°ï¼šèåˆã€BNåã€åŸå§‹ReIDç‰¹å¾
        fused_norms = None
        bn_norms = None
        reid_raw_norms = None
        if isinstance(outputs, dict):
            if 'features' in outputs:
                fused_norms = torch.norm(outputs['features'].detach(), p=2, dim=1)  # è¢«feat_penaltyçº¦æŸçš„é‡
            if 'bn_features' in outputs:
                bn_norms = torch.norm(outputs['bn_features'].detach(), p=2, dim=1)  # BNåç‰¹å¾ï¼ˆå¯¹é½+æ£€ç´¢ç”¨ï¼‰
            if 'reid_features_raw' in outputs:
                reid_raw_norms = torch.norm(outputs['reid_features_raw'].detach(), p=2, dim=1)  # å—LayerNormå½±å“

        # ä½¿ç”¨BNç‰¹å¾èŒƒæ•°æ›´æ–°ç´¯ç§¯ç»Ÿè®¡ï¼ˆå› ä¸ºè¿™æ˜¯å¯¹é½å’Œæ£€ç´¢çš„å…³é”®ï¼‰
        if bn_norms is not None:
            feature_norms.extend(bn_norms.cpu().numpy())
        elif fused_norms is not None:
            feature_norms.extend(fused_norms.cpu().numpy())

        # è¿›åº¦æ¡æ˜¾ç¤ºBNåç‰¹å¾èŒƒæ•°ï¼ˆæœ€é‡è¦çš„ç›‘æ§æŒ‡æ ‡ï¼‰
        avg_fused = float(fused_norms.mean().item()) if fused_norms is not None else 0.0
        avg_bn = float(bn_norms.mean().item()) if bn_norms is not None else 0.0
        avg_reid  = float(reid_raw_norms.mean().item()) if reid_raw_norms is not None else 0.0
        avg_grad_norm = (np.mean(grad_norms[-10:]) if grad_norms else None)

        # åŒºåˆ†SDMæŸå¤±å’Œåˆ†æ•°è¿›è¡Œç›‘æ§
        sdm_loss_val = float(sdm_loss.item())
        
        # å…³é”®ç›‘æ§å­—æ®µè¾“å‡ºï¼ˆæŒ‰ä½ çš„å»ºè®®æ ¼å¼ï¼‰
        if batch_idx % 50 == 0:  # æ¯50ä¸ªbatchè¾“å‡ºä¸€æ¬¡å…³é”®ç›‘æ§
            # è®¡ç®—æœ‰æ•ˆæ ·æœ¬ç»Ÿè®¡
            feature_masks = outputs.get('feature_masks', {})
            vis_cnt = 0
            for mod_name, mask in feature_masks.items():
                if mask is not None:
                    mod_valid = (mask > 0).squeeze(-1) if mask.dim() > 1 else (mask > 0)
                    if mod_name in ['rgb', 'vis']:  # RGBæˆ–å¯è§å…‰æ¨¡æ€
                        vis_cnt = int(mod_valid.sum())
                        break
            
            # ä»loss_dictä¸­è·å–ce_validè®¡æ•°
            ce_valid_cnt = loss_dict.get('ce_valid_cnt', len(labels))  # ä»æ¨¡å‹è·å–å®é™…æœ‰æ•ˆCEæ ·æœ¬æ•°
            
            # è¾“å‡ºå…³é”®å­—æ®µ
            logging.info(f"[{epoch:02d}, {batch_idx:03d}] "
                        f"vis_cnt={vis_cnt}, ce_valid={ce_valid_cnt}, "
                        f"Feat(BN)_mean={avg_bn:.2f}, "
                        f"SDMLoss={sdm_loss_val:.3f}, CE={float(ce_loss.item()):.3f}")
        
        # æ—©æœŸè®­ç»ƒè¯¦ç»†ç›‘æ§ï¼ˆå‰3ä¸ªepochï¼Œæ¯20ä¸ªbatchï¼‰
        if epoch <= 3 and batch_idx % 20 == 0:
            # è·å–logitsçš„æœ€å¤§ç»å¯¹å€¼
            max_abs_logit = 0.0
            if 'logits' in outputs:
                max_abs_logit = float(outputs['logits'].abs().max().item())
            
            logging.info(f"æ•°å€¼ç›‘æ§ Epoch {epoch}, Batch {batch_idx}: "
                        f"max_abs_logit={max_abs_logit:.2f}, "
                        f"SDM_weight={effective_cont_w:.3f}, SDM_temp={current_temp:.3f}")
            
            # æ£€æŸ¥SDMæŸå¤±å¼‚å¸¸ï¼ˆä¿®å¤ååº”è¯¥å¤©ç„¶éè´Ÿï¼‰
            if sdm_loss_val < 0:
                logging.warning(f"âš ï¸ SDMæŸå¤±å¼‚å¸¸ä¸ºè´Ÿå€¼: {sdm_loss_val:.4f} - æ£€æŸ¥maskè¿‡æ»¤æ˜¯å¦ç”Ÿæ•ˆï¼")
            elif sdm_loss_val > 5.0:
                logging.warning(f"âš ï¸ SDMæŸå¤±è¿‡å¤§: {sdm_loss_val:.4f} - å¯èƒ½å­˜åœ¨æ•°å€¼ä¸ç¨³å®š")
        
        # BNç‰¹å¾èŒƒæ•°è­¦å‘Šï¼šå‰5ä¸ªepochä¸æŠ¥è­¦ï¼Œç»™æ­£åˆ™åŒ–æ—¶é—´ç”Ÿæ•ˆ
        if avg_bn > 12.0 and epoch > 5 and batch_idx % 50 == 0:
            logging.warning(f"âš ï¸  BNç‰¹å¾èŒƒæ•°è¿‡å¤§: {avg_bn:.2f} - æ­£åˆ™åŒ–æœªç”Ÿæ•ˆ (Epoch {epoch})")
        
        # ä¼˜åŒ–ï¼šæ¯5ä¸ªbatchæ›´æ–°ä¸€æ¬¡è¿›åº¦æ¡ï¼Œé¿å…é¢‘ç¹GPU-CPUåŒæ­¥
        if batch_idx % 5 == 0:
            pbar.set_postfix({
                'Loss': f'{current_loss:.3f}',
                'CE': f'{float(ce_loss.item()):.3f}',
                'SDMLoss': f'{sdm_loss_val:.3f}',  # ä¿®å¤ååº”è¯¥éè´Ÿ
                'Feat(BN)': f'{avg_bn:.2f}',  # é‡ç‚¹ç›‘æ§BNåç‰¹å¾èŒƒæ•°
                'GradNorm': ('â€”' if avg_grad_norm is None else f'{avg_grad_norm:.2f}'),
                'Spikes': loss_spikes
            })

    avg_loss = total_loss / max(1, len(dataloader))
    accuracy = 100. * correct / max(1, total)
    avg_feat_norm = np.mean(feature_norms) if feature_norms else 0.0
    avg_grad_norm = np.mean(grad_norms) if grad_norms else 0.0
    
    if loss_spikes > len(dataloader) * 0.1:
        logging.warning(f"Epoch {epoch}: æŸå¤±å¼‚å¸¸æ¬¡æ•° {loss_spikes}ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡")
    if avg_feat_norm > 50.0:
        logging.warning(f"Epoch {epoch}: å¹³å‡ç‰¹å¾èŒƒæ•° {avg_feat_norm:.2f} åå¤§ï¼Œæ£€æŸ¥ç¨³å®šæ€§")
    
    # è®­ç»ƒå®Œæˆåæ¸…ç†CUDAç¼“å­˜
    torch.cuda.empty_cache()
    
    return {
        'total_loss': avg_loss,
        'ce_loss': ce_loss_sum / max(1, len(dataloader)),
        'contrastive_loss': contrastive_loss_sum / max(1, len(dataloader)),  # å…¼å®¹æ€§ï¼šå®é™…ä¸ºSDMæŸå¤±
        'sdm_loss': contrastive_loss_sum / max(1, len(dataloader)),  # æ˜ç¡®çš„SDMæŸå¤±
        'accuracy': accuracy,
        'feature_norm': avg_feat_norm,
        'grad_norm': avg_grad_norm,
        'loss_spikes': loss_spikes,
        'stability_score': max(0.0, 1.0 - train_epoch._spike_state['spikes'] / max(1, train_epoch._spike_state['batches']))
    }

# ------------------------------
# è®­ç»ƒä¸»æµç¨‹
# ------------------------------
def _build_lambda_with_warmup_cosine(total_epochs, warmup_epochs, start_factor=0.01, min_factor=0.01):
    """è¿”å›ä¸€ä¸ª epoch->scale çš„å‡½æ•°ï¼šwarmupï¼ˆçº¿æ€§ï¼‰åæ¥ cosine è¡°å‡ï¼›ä¿æŒ param group æ¯”ä¾‹"""
    assert 0.0 < start_factor <= 1.0
    assert 0.0 < min_factor <= 1.0
    def lmbda(epoch: int):
        if epoch < warmup_epochs:
            return start_factor + (1.0 - start_factor) * (epoch + 1) / max(1, warmup_epochs)
        # cosine
        T = max(1, total_epochs - warmup_epochs)
        t = max(0, epoch - warmup_epochs)
        cos = 0.5 * (1.0 + math.cos(math.pi * t / T))
        return min_factor + (1.0 - min_factor) * cos
    return lmbda

def train_multimodal_reid():
    # å¯ç”¨TF32åŠ é€Ÿï¼ˆA100/RTX30xx/40xxç³»åˆ—GPUï¼‰
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        logging.info("å·²å¯ç”¨TF32åŠ é€Ÿ")
    
    # é…ç½®ä¸è®¾å¤‡
    config = TrainingConfig()
    set_seed(getattr(config, "seed", 42))

    device_str = getattr(config, "device", "cuda" if torch.cuda.is_available() else "cpu")
    device = torch.device(device_str if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    setup_logging(getattr(config, "log_dir", "./logs"))

    # åŠ è½½å®Œæ•´æ•°æ®é›†å¹¶è¿›è¡Œèº«ä»½åˆ’åˆ†
    logging.info("åŠ è½½æ•°æ®é›†å¹¶è¿›è¡Œèº«ä»½åˆ’åˆ†...")
    full_dataset = MultiModalDataset(config, split='train')

    # ä½¿ç”¨æ–°çš„åˆ’åˆ†å·¥å…·
    from tools.split import split_ids, create_split_datasets, verify_split_integrity
    
    # è·å–æ‰€æœ‰äººå‘˜ID
    all_person_ids = [full_dataset.data_list[i]['person_id'] for i in range(len(full_dataset))]
    all_person_ids = sorted(list(set(all_person_ids)))
    
    # åˆ›å»ºperson_idåˆ°æ ‡ç­¾çš„æ˜ å°„
    pid2label = {pid: idx for idx, pid in enumerate(all_person_ids)}
    
    # æŒ‰IDåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_ids, val_ids = split_ids(
        all_person_ids, 
        val_ratio=getattr(config, "val_ratio", 0.2),
        seed=getattr(config, "seed", 42)
    )
    
    # åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_dataset, local_val_dataset = create_split_datasets(
        full_dataset, train_ids, val_ids, config
    )
    
    # éªŒè¯åˆ’åˆ†å®Œæ•´æ€§
    verify_split_integrity(train_dataset, local_val_dataset)
    
    # è¾“å‡ºæœ€ç»ˆçš„æ•°æ®é›†ç»Ÿè®¡ï¼ˆç¡®è®¤åˆ’åˆ†æˆåŠŸï¼‰
    train_ids_actual = set(item['person_id'] for item in train_dataset.data_list)
    val_ids_actual = set(item['person_id'] for item in local_val_dataset.data_list)
    
    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†ç»“æœ:")
    print(f"  åŸå§‹æ•°æ®é›†: {len(full_dataset.data_list)} æ ·æœ¬, {len(all_person_ids)} ä¸ªID")
    print(f"  è®­ç»ƒæ•°æ®é›†: {len(train_dataset.data_list)} æ ·æœ¬, {len(train_ids_actual)} ä¸ªID ({len(train_dataset.data_list)/len(full_dataset.data_list):.1%})")
    print(f"  éªŒè¯æ•°æ®é›†: {len(local_val_dataset.data_list)} æ ·æœ¬, {len(val_ids_actual)} ä¸ªID ({len(local_val_dataset.data_list)/len(full_dataset.data_list):.1%})")
    print(f"  IDé‡å æ£€æŸ¥: {'âœ… æ— é‡å ' if len(train_ids_actual & val_ids_actual) == 0 else 'âŒ å­˜åœ¨é‡å '}")
    
    logging.info(f"æœ€ç»ˆæ•°æ®é›†: è®­ç»ƒé›†{len(train_dataset.data_list)}æ ·æœ¬, éªŒè¯é›†{len(local_val_dataset.data_list)}æ ·æœ¬")

    # åˆ†ç±»å¤´ num_classesï¼ˆåº”è¯¥è¦†ç›–æ‰€æœ‰å¯èƒ½çš„person_idä»¥é¿å…æ ‡ç­¾è¶…å‡ºèŒƒå›´ï¼‰
    config.num_classes = len(all_person_ids)
    logging.info(f"åˆ†ç±»å™¨è¾“å‡ºç»´åº¦: {config.num_classes} ç±»")
    logging.info(f"æ ‡ç­¾æ˜ å°„èŒƒå›´: 0-{len(all_person_ids)-1} ï¼ˆåŒ…å«è®­ç»ƒé›†+éªŒè¯é›†ï¼‰")
    train_labels = [pid2label[pid] for pid in train_ids]
    val_labels = [pid2label[pid] for pid in val_ids]
    logging.info(f"è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´: {min(train_labels)}-{max(train_labels)}, éªŒè¯é›†æ ‡ç­¾èŒƒå›´: {min(val_labels)}-{max(val_labels)}")

    # ==== batch size ç»Ÿä¸€å®šä¹‰ï¼ˆå¿…é¡»åœ¨é¦–æ¬¡ä½¿ç”¨å‰è®¡ç®—ï¼‰====
    world_size = (torch.distributed.get_world_size() 
                  if torch.distributed.is_available() and torch.distributed.is_initialized() else 1)
    grad_accum_steps = getattr(config, "gradient_accumulation_steps", 4)
    
    # config.batch_size çº¦å®šä¸º"æ¯å¡ micro-batch"
    actual_batch_size = config.batch_size                   # per-GPU micro batch
    effective_batch_size = actual_batch_size * grad_accum_steps * world_size  # å…¨å±€æœ‰æ•ˆ batch
    
    logging.info(f"Batch size é…ç½®: micro={actual_batch_size}, ç´¯ç§¯æ­¥æ•°={grad_accum_steps}, ç­‰æ•ˆ={effective_batch_size}")

    # è®­ç»ƒ DataLoaderï¼ˆè°ƒæ•´ PÃ—K ä»¥é€‚åº”æ¢¯åº¦ç´¯ç§¯ï¼‰
    num_instances = 4  # K=4ï¼Œæ¯ä¸ªèº«ä»½4ä¸ªæ ·æœ¬ï¼Œç¡®ä¿æ¯ä¸ªé”šæœ‰K-1=3ä¸ªæ­£æ ·æœ¬
    # ä½¿ç”¨ micro batch sizeï¼ˆå•æ­¥å®é™…å¤„ç†çš„æ ·æœ¬æ•°ï¼‰
    num_pids_per_batch = actual_batch_size // num_instances  # è°ƒæ•´åçš„P
    logging.info(f"é‡‡æ ·ç­–ç•¥: PÃ—K = {num_pids_per_batch}Ã—{num_instances} = {actual_batch_size}")
    logging.info(f"æ¯ä¸ªé”šçš„æ­£æ ·æœ¬æ•°: {num_instances-1}")
    
    train_sampler = BalancedBatchSampler(train_dataset, actual_batch_size, num_instances=num_instances)
    train_loader = DataLoader(
        train_dataset,
        batch_sampler=train_sampler,
        num_workers=getattr(config, "num_workers", 4),
        pin_memory=getattr(config, "pin_memory", True),
        persistent_workers=True,  # ä¿æŒå·¥ä½œè¿›ç¨‹æŒç»­è¿è¡Œ
        prefetch_factor=2,        # é¢„å–å› å­æå‡IOæ•ˆç‡
        collate_fn=compatible_collate_fn
    )

    # æœ¬åœ°éªŒè¯ DataLoaderï¼ˆèµ›åˆ¶å¯¹é½ï¼‰
    gallery_loader, query_loaders = build_eval_loaders_by_rule(
        local_val_dataset, list(range(len(local_val_dataset))),
        batch_size=actual_batch_size,  # éªŒè¯æ—¶ä½¿ç”¨ micro batch size
        num_workers=getattr(config, "num_workers", 4),
        pin_memory=getattr(config, "pin_memory", True)
    )

    # æ¨¡å‹ï¼šCLIP+MERæ¶æ„
    model = CLIPBasedMultiModalReIDModel(config).to(device)
    
    # è®¾ç½®åˆ†ç±»å™¨ï¼ˆåŠ¨æ€è®¾ç½®IDç±»åˆ«æ•°ï¼‰
    num_classes = getattr(config, 'num_classes', None)
    if num_classes is not None:
        model.set_num_classes(num_classes)
        logging.info(f"è®¾ç½®åˆ†ç±»å™¨ï¼š{num_classes} ä¸ªIDç±»åˆ«")
    else:
        logging.warning("configä¸­æœªæ‰¾åˆ°num_classesï¼Œè¯·ç¡®ä¿åœ¨æ•°æ®åŠ è½½åè®¾ç½®")

    # æ˜¾å­˜ä¼˜åŒ–ï¼šå†»ç»“ä¸»å¹²ï¼Œåªè®­ç»ƒ LoRA å’Œç‰¹å®šæ¨¡å—
    freeze_backbone = getattr(config, 'freeze_backbone', True)
    if freeze_backbone:
        logging.info("å†»ç»“ CLIP ä¸»å¹²ï¼Œåªè®­ç»ƒ LoRA å’Œç‰¹å®šæ¨¡å—")
        for name, param in model.named_parameters():
            if 'loras' in name or 'feature_mixture' in name or 'bn_neck' in name or 'null_tokens' in name:
                param.requires_grad = True
            else:
                param.requires_grad = False
    
    # ä¼˜åŒ–å™¨ - æ”¯æŒCLIP+MERåˆ†å±‚å­¦ä¹ ç‡
    param_groups = model.get_learnable_params()
    
    # è¿‡æ»¤æ‰å†»ç»“çš„å‚æ•°
    filtered_param_groups = []
    for group in param_groups:
        trainable_params = [p for p in group['params'] if p.requires_grad]
        if trainable_params:  # åªæœ‰åŒ…å«å¯è®­ç»ƒå‚æ•°çš„ç»„æ‰æ·»åŠ 
            group['params'] = trainable_params
            filtered_param_groups.append(group)
    
    # æ—¥å¿—è¾“å‡ºå„å‚æ•°ç»„çš„å­¦ä¹ ç‡
    total_trainable = 0
    for group in filtered_param_groups:
        group_name = group.get('name', 'unknown')
        group_lr = group['lr']
        num_params = len(group['params'])
        total_trainable += sum(p.numel() for p in group['params'])
        logging.info(f"{group_name}: {num_params} å‚æ•°, å­¦ä¹ ç‡: {group_lr:.2e}")
    
    logging.info(f"å¯è®­ç»ƒå‚æ•°æ€»æ•°: {total_trainable:,}")
    
    optimizer = AdamW(filtered_param_groups, weight_decay=getattr(config, "weight_decay", 1e-4))

    # AMP ä¼˜åŒ–ï¼šä½¿ç”¨ bfloat16 + æ¢¯åº¦ç´¯ç§¯
    autocast_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    use_scaler = (autocast_dtype == torch.float16)  # bfloat16 ä¸éœ€è¦ GradScaler
    scaler = GradScaler(enabled=use_scaler) if use_scaler else None
    
    # ä½¿ç”¨å‰é¢ç»Ÿä¸€å®šä¹‰çš„ batch size å‚æ•°
    accum_steps = grad_accum_steps  # ä½¿ç”¨å‰é¢å®šä¹‰çš„å˜é‡
    
    logging.info(f"æ··åˆç²¾åº¦: {autocast_dtype}, æ¢¯åº¦ç´¯ç§¯: {accum_steps} æ­¥")
    logging.info(f"å®é™… batch_size: {actual_batch_size}, ç­‰æ•ˆ batch_size: {effective_batch_size}")

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    warmup_epochs = getattr(config, "warmup_epochs", 15)
    scheduler_type = getattr(config, "scheduler", "cosine")
    num_epochs = getattr(config, "num_epochs", 100)

    if scheduler_type == 'cosine':
        # ä½¿ç”¨ LambdaLR ä¿æŒ param group æ¯”ä¾‹
        start_factor = 0.01
        min_factor = 0.01
        lmbda = _build_lambda_with_warmup_cosine(num_epochs, warmup_epochs, start_factor, min_factor)
        scheduler = LambdaLR(optimizer, lr_lambda=[lmbda] * len(optimizer.param_groups))
        logging.info(f"è°ƒåº¦å™¨: Warmup({warmup_epochs}) + Cosine(min_factor={min_factor}) via LambdaLR")
    elif scheduler_type == 'plateau':
        base_lr = getattr(config, 'base_learning_rate', 1e-5)
        scheduler = ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=8, threshold=0.001,
            min_lr=base_lr * 0.001, verbose=True
        )
        logging.info("è°ƒåº¦å™¨: ReduceLROnPlateau (åŸºäºmAP)")
    elif scheduler_type == 'step':
        step_size = int(50 * getattr(config, "conservative_factor", 0.7))
        step_size = max(step_size, 30)
        gamma = 0.3 + 0.4 * getattr(config, "conservative_factor", 0.7)
        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)
        logging.info(f"è°ƒåº¦å™¨: StepLR(step_size={step_size}, gamma={gamma:.2f})")
    elif scheduler_type == 'multistep':
        cf = getattr(config, "conservative_factor", 0.7)
        milestones = [int(60 * cf), int(80 * cf), int(95 * cf)]
        milestones = [max(m, 30) for m in milestones]
        gamma = 0.2 + 0.5 * cf
        scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=gamma)
        logging.info(f"è°ƒåº¦å™¨: MultiStepLR(milestones={milestones}, gamma={gamma:.2f})")
    else:
        scheduler = None
        logging.info("è°ƒåº¦å™¨: None")

    # è®­ç»ƒå¾ªç¯
    best_map = 0.0
    train_history, val_history = [], []
    eval_freq = getattr(config, "eval_freq", 20)
    save_dir = getattr(config, "save_dir", "./checkpoints")
    os.makedirs(save_dir, exist_ok=True)

    for epoch in range(1, num_epochs + 1):
        start_time = time.time()
        
        # æ¯ä¸ªepochå¼€å§‹å‰æ¸…é™¤æ–‡æœ¬ç¼“å­˜ï¼ˆä»…åœ¨å¾®è°ƒæ–‡æœ¬æ—¶æ¸…é™¤ï¼Œå†»ç»“æ—¶ä¿æŒç¼“å­˜ï¼‰
        if hasattr(model, "text_cache") and not getattr(model, "freeze_text", True):
            model.text_cache.clear()
            if epoch == 1:  # ç¬¬ä¸€ä¸ªepochæé†’
                logging.info("æ–‡æœ¬ç¼–ç å™¨å¾®è°ƒæ¨¡å¼ï¼šæ¯epochæ¸…é™¤ç¼“å­˜")

        adaptive_clip = getattr(config, "adaptive_gradient_clip", True)
        train_metrics = train_epoch(model, train_loader, optimizer, device, epoch, scaler, adaptive_clip, accum_steps, autocast_dtype)
        
        # === SDMè°ƒåº¦å™¨æ›´æ–°ï¼ˆåŸºäºè®­ç»ƒæŒ‡æ ‡ï¼‰ ===
        if hasattr(model, 'sdm_scheduler'):
            # æ›´æ–°SDMå‚æ•°
            effective_cont_w, current_temp = model.sdm_scheduler.get_parameters(epoch, train_metrics, comp_metrics if 'comp_metrics' in locals() else None)
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¢åŠ æƒé‡
            if model.sdm_scheduler.can_increase_weight(epoch, train_metrics, comp_metrics if 'comp_metrics' in locals() else None):
                if model.sdm_scheduler.increase_weight():
                    effective_cont_w = model.sdm_scheduler.weight_scheduler.current_weight
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é™ä½æƒé‡ï¼ˆå¼‚å¸¸æƒ…å†µï¼‰
            current_sdm_loss = train_metrics.get('sdm_loss', train_metrics.get('contrastive_loss', 0.0))
            if current_sdm_loss > 5.0 or current_sdm_loss < 0:
                model.sdm_scheduler.decrease_weight(f"SDMæŸå¤±å¼‚å¸¸: {current_sdm_loss:.4f}")
                effective_cont_w = model.sdm_scheduler.weight_scheduler.current_weight
        
        # è‡ªé€‚åº”å¢å¼ºå¼ºåº¦ï¼š5ä¸ªepochåå¦‚æœè®­ç»ƒç¨³å®šï¼Œæ”¾å®½è£å‰ªå¼ºåº¦
        if epoch == 5 and train_metrics['stability_score'] > 0.8:
            logging.info("è®­ç»ƒç¨³å®šï¼Œæ”¾å®½æ•°æ®å¢å¼ºå¼ºåº¦ï¼šè£å‰ª scale (0.8,1.0) -> (0.6,1.0)")
            # æ›´æ–°æ•°æ®å¢å¼ºå˜æ¢
            new_transform = transforms.Compose([
                transforms.RandomResizedCrop(config.image_size, scale=(0.6, 1.0)),  # æ”¾å®½è£å‰ª
                transforms.RandomHorizontalFlip(0.5),
                transforms.ColorJitter(brightness=0.2, contrast=0.2) if config.color_jitter else transforms.Lambda(lambda x: x),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                transforms.RandomErasing(p=config.random_erase, scale=(0.02, 0.2)) if config.random_erase > 0 else transforms.Lambda(lambda x: x)
            ])
            # æ›´æ–°è®­ç»ƒæ•°æ®é›†çš„å˜æ¢
            if hasattr(train_dataset, 'transform'):
                train_dataset.transform = new_transform


        if epoch % eval_freq == 0:
            sample_ratio = getattr(config, "eval_sample_ratio", 0.3)
            comp_metrics = validate_competition_style(model, gallery_loader, query_loaders, device, k_map=100, sample_ratio=sample_ratio)

            train_history.append({'epoch': epoch, **train_metrics})
            val_history.append({'epoch': epoch, **comp_metrics})

            score = comp_metrics['map_avg2']
            if score > best_map:
                best_map = score
                save_checkpoint(
                    model, optimizer, scheduler, epoch, best_map, config,
                    os.path.join(save_dir, 'best_model.pth')
                )
                logging.info(f"æ–°çš„æœ€ä½³(single+quadå¹³å‡) mAP: {best_map:.4f}")

            # è¾“å‡ºåˆ†æ¨¡æ€mAPè¯¦æƒ…ï¼Œå¿«é€Ÿå®šä½æ‹–åè…¿çš„æ¨¡æ€
            single_detail = comp_metrics.get('detail', {}).get('single', {})
            single_maps = []
            for modality in ['text', 'nir', 'sk', 'cp']:
                if modality in single_detail:
                    single_maps.append(f"{modality}:{single_detail[modality]:.3f}")
                    
            single_detail_str = " | ".join(single_maps) if single_maps else "N/A"
            
            logging.info(
                f"Epoch {epoch}/{num_epochs} - "
                f"Train ClsAcc: {train_metrics['accuracy']:.2f}% - "
                f"mAP(single/quad/avg2): "
                f"{comp_metrics['map_single']:.4f}/"
                f"{comp_metrics['map_quad']:.4f}/"
                f"{comp_metrics['map_avg2']:.4f} - "
                f"CMC@1/5/10: {comp_metrics['cmc1']:.4f}/"
                f"{comp_metrics['cmc5']:.4f}/"
                f"{comp_metrics['cmc10']:.4f} - "
                f"ç”¨æ—¶: {time.time() - start_time:.2f}s"
            )
            logging.info(f"åˆ†æ¨¡æ€mAPè¯¦æƒ…: {single_detail_str}")

        # ä¿å­˜ checkpoint
        if epoch % getattr(config, "save_freq", 20) == 0:
            save_checkpoint(
                model, optimizer, scheduler, epoch, best_map, config,
                os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth')
            )

        # è°ƒåº¦å™¨æ­¥è¿›
        if scheduler:
            if isinstance(scheduler, ReduceLROnPlateau):
                if epoch % eval_freq == 0:
                    current_map = comp_metrics['map_avg2'] if 'comp_metrics' in locals() else 0.0
                    scheduler.step(current_map)
            else:
                scheduler.step()

            if epoch % 20 == 0:  # å‡å°‘æŸ¥è¯¢é¢‘ç‡ï¼š10->20
                lrs = [pg['lr'] for pg in optimizer.param_groups]
                logging.info(f"Epoch {epoch}: å½“å‰å­¦ä¹ ç‡ = {', '.join([f'{lr:.2e}' for lr in lrs])}")

    # è®­ç»ƒå®Œæˆåå…¨é‡è¯„ä¼°
    logging.info("è®­ç»ƒå®Œæˆï¼Œå¼€å§‹æœ¬åœ°åˆ’åˆ†éªŒè¯é›†çš„å®Œæ•´è¯„ä¼°...")
    final_metrics = validate_competition_style(model, gallery_loader, query_loaders, device, k_map=100, sample_ratio=1.0)
    # æœ€ç»ˆè¯„ä¼°çš„åˆ†æ¨¡æ€mAPè¯¦æƒ…
    final_single_detail = final_metrics.get('detail', {}).get('single', {})
    final_single_maps = []
    for modality in ['text', 'nir', 'sk', 'cp']:
        if modality in final_single_detail:
            final_single_maps.append(f"{modality}:{final_single_detail[modality]:.3f}")
    final_single_detail_str = " | ".join(final_single_maps) if final_single_maps else "N/A"
    
    logging.info(f"æœ€ç»ˆè¯„ä¼° - mAP(single/quad/avg2): "
                f"{final_metrics['map_single']:.4f}/"
                f"{final_metrics['map_quad']:.4f}/"
                f"{final_metrics['map_avg2']:.4f}")
    logging.info(f"æœ€ç»ˆåˆ†æ¨¡æ€mAPè¯¦æƒ…: {final_single_detail_str}")
    
    # æ‰¾å‡ºæ‹–åè…¿çš„æ¨¡æ€
    if final_single_detail:
        min_modality = min(final_single_detail.items(), key=lambda x: x[1])
        max_modality = max(final_single_detail.items(), key=lambda x: x[1])
        logging.info(f"æ€§èƒ½æœ€å·®æ¨¡æ€: {min_modality[0]}({min_modality[1]:.3f}), æœ€ä½³æ¨¡æ€: {max_modality[0]}({max_modality[1]:.3f})")

    # ä¿å­˜å†å²
    log_dir = getattr(config, "log_dir", "./logs")
    os.makedirs(log_dir, exist_ok=True)
    pd.DataFrame(train_history).to_csv(os.path.join(log_dir, 'train_history.csv'), index=False)
    pd.DataFrame(val_history).to_csv(os.path.join(log_dir, 'local_val_history.csv'), index=False)
    final_results = {'epoch': 'final', **final_metrics}
    pd.DataFrame([final_results]).to_csv(os.path.join(log_dir, 'local_val_final_evaluation.csv'), index=False)

    # ä¿å­˜åˆ’åˆ†
    split_info = {
        'train_ids': train_ids,
        'val_ids': val_ids,
        'train_indices': train_indices,
        'val_indices': val_indices
    }
    with open(os.path.join(save_dir, 'dataset_split.pkl'), 'wb') as f:
        pickle.dump(split_info, f)

    logging.info(f"è®­ç»ƒå®Œæˆ. æœ¬åœ°åˆ’åˆ†éªŒè¯é›†æœ€ä½³(å››ç±»å¹³å‡) mAP: {best_map:.4f}")

def save_checkpoint(model, optimizer, scheduler, epoch, best_map, config, filename):
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'best_map': best_map,
        'num_classes': getattr(config, 'num_classes', None),
        'config': config.__dict__ if hasattr(config, '__dict__') else str(config)
    }
    torch.save(checkpoint, filename)

if __name__ == "__main__":
    train_multimodal_reid()
