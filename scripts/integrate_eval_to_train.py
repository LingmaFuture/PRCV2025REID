# tools/integrate_eval_to_train.py
"""
é›†æˆè¯„ä¼°åè®®åˆ°è®­ç»ƒæµç¨‹
åœ¨è®­ç»ƒå®Œæˆåè‡ªåŠ¨è¿è¡Œå¤šæ¨¡æ€è¯„ä¼°åè®®
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.eval_mm_protocol import run_eval
from configs.config import TrainingConfig

def run_post_training_evaluation(
    dataset_root: str = "./data/train",
    model_path: str = "./checkpoints/best_model.pth",
    cache_dir: str = "./eval_cache",
    seed: int = 42,
    device: str = "cuda"
):
    """
    è®­ç»ƒåè¯„ä¼°å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥åœ¨train.pyä¸­è°ƒç”¨
    
    Args:
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•
        model_path: æœ€ä½³æ¨¡å‹æƒé‡è·¯å¾„
        cache_dir: è¯„ä¼°ç¼“å­˜ç›®å½•
        seed: éšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ¯ å¼€å§‹è¿è¡Œå¤šæ¨¡æ€è¯„ä¼°åè®®ï¼ˆMM-1/2/3/4ï¼‰")
    print("="*60)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·ç¡®ä¿è®­ç»ƒå·²å®Œæˆå¹¶ä¿å­˜äº†æœ€ä½³æ¨¡å‹")
        return None
    
    try:
        # è¿è¡Œè¯„ä¼°åè®®
        results = run_eval(
            dataset_root=dataset_root,
            model_path=model_path,
            cache_dir=cache_dir,
            seed=seed,
            device=device
        )
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        import json
        result_path = os.path.join(os.path.dirname(model_path), "mm_evaluation_results.json")
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_path}")
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“ˆ è¯„ä¼°æ€»ç»“")
        print("="*60)
        
        avg_results = results.get("AVG(1-4)", {})
        print(f"å¹³å‡ mAP: {avg_results.get('mAP', 0.0):.4f}")
        print(f"å¹³å‡ R@1: {avg_results.get('R@1', 0.0):.4f}")
        print(f"å¹³å‡ R@5: {avg_results.get('R@5', 0.0):.4f}")
        print(f"å¹³å‡ R@10: {avg_results.get('R@10', 0.0):.4f}")
        
        print("\nåˆ†æ¨¡æ€è¯¦ç»†ç»“æœ:")
        for mode in ["MM-1", "MM-2", "MM-3", "MM-4"]:
            if mode in results:
                mode_res = results[mode]
                print(f"{mode}: mAP={mode_res.get('mAP', 0.0):.4f}, "
                      f"R@1={mode_res.get('R@1', 0.0):.4f}, "
                      f"æŸ¥è¯¢æ•°={mode_res.get('num_queries', 0)}")
        
        print("="*60)
        
        return results
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

def add_evaluation_to_train_script():
    """
    æä¾›ä»£ç ç‰‡æ®µï¼Œå¯ä»¥æ·»åŠ åˆ°train.pyçš„æœ«å°¾
    """
    code_snippet = '''
# ===== è®­ç»ƒå®Œæˆåè¿è¡Œå¤šæ¨¡æ€è¯„ä¼°åè®® =====
def run_final_evaluation():
    """è®­ç»ƒå®Œæˆåçš„æœ€ç»ˆè¯„ä¼°"""
    print("\\n" + "="*60)
    print("ğŸ¯ å¼€å§‹æœ€ç»ˆå¤šæ¨¡æ€è¯„ä¼°åè®®")
    print("="*60)
    
    try:
        from tools.integrate_eval_to_train import run_post_training_evaluation
        
        # è¿è¡Œè¯„ä¼°
        eval_results = run_post_training_evaluation(
            dataset_root=config.data_root,
            model_path=os.path.join(config.save_dir, 'best_model.pth'),
            cache_dir=os.path.join(config.save_dir, 'eval_cache'),
            seed=config.seed,
            device=str(device)
        )
        
        if eval_results:
            # å°†è¯„ä¼°ç»“æœæ·»åŠ åˆ°è®­ç»ƒå†å²
            final_eval_summary = {
                'final_mm_avg_map': eval_results.get("AVG(1-4)", {}).get('mAP', 0.0),
                'final_mm_avg_r1': eval_results.get("AVG(1-4)", {}).get('R@1', 0.0),
                'mm1_map': eval_results.get("MM-1", {}).get('mAP', 0.0),
                'mm2_map': eval_results.get("MM-2", {}).get('mAP', 0.0),
                'mm3_map': eval_results.get("MM-3", {}).get('mAP', 0.0),
                'mm4_map': eval_results.get("MM-4", {}).get('mAP', 0.0),
            }
            
            # ä¿å­˜åˆ°CSV
            import pandas as pd
            final_df = pd.DataFrame([final_eval_summary])
            final_df.to_csv(os.path.join(log_dir, 'final_mm_evaluation.csv'), index=False)
            
            logging.info(f"âœ… æœ€ç»ˆå¤šæ¨¡æ€è¯„ä¼°å®Œæˆï¼å¹³å‡mAP: {final_eval_summary['final_mm_avg_map']:.4f}")
            
        else:
            logging.warning("âš ï¸ æœ€ç»ˆè¯„ä¼°å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œè¯„ä¼°è„šæœ¬")
            
    except Exception as e:
        logging.error(f"âŒ æœ€ç»ˆè¯„ä¼°å‡ºé”™: {e}")

# åœ¨train_multimodal_reid()å‡½æ•°çš„æœ€åæ·»åŠ ï¼š
if __name__ == "__main__":
    # åŸæœ‰çš„è®­ç»ƒä»£ç ...
    train_multimodal_reid()
    
    # è®­ç»ƒå®Œæˆåè¿è¡Œæœ€ç»ˆè¯„ä¼°
    try:
        run_final_evaluation()
    except Exception as e:
        print(f"æœ€ç»ˆè¯„ä¼°å¤±è´¥: {e}")
        print("å¯ä»¥æ‰‹åŠ¨è¿è¡Œ: python tools/eval_mm_protocol.py --model_path ./checkpoints/best_model.pth")
'''
    
    return code_snippet

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šè¿è¡Œè¯„ä¼°
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒåè¯„ä¼°")
    parser.add_argument("--dataset_root", type=str, default="./data/train", help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--model_path", type=str, default="./checkpoints/best_model.pth", help="æ¨¡å‹æƒé‡è·¯å¾„")
    parser.add_argument("--cache_dir", type=str, default="./eval_cache", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    
    args = parser.parse_args()
    
    # è¿è¡Œè¯„ä¼°
    results = run_post_training_evaluation(
        dataset_root=args.dataset_root,
        model_path=args.model_path,
        cache_dir=args.cache_dir,
        seed=args.seed,
        device=args.device
    )
    
    if results:
        print("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    else:
        print("âŒ è¯„ä¼°å¤±è´¥ï¼")
