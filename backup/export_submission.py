# tools/export_submission.py
"""
å¯¼å‡ºKaggleæäº¤CSVæ–‡ä»¶
åŸºäºå¤šæ¨¡æ€è¯„ä¼°åè®®ç”Ÿæˆç«èµ›æäº¤æ ¼å¼
"""

import os
import sys
import json
import random
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
import torch
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.eval_mm_protocol import (
    build_index, build_gallery, build_queries, 
    FeatureExtractor, extract_gallery_feats,
    extract_query_feat, cosine_sim, l2n
)
from datasets.dataset import MultiModalDataset
from models.model import CLIPBasedMultiModalReIDModel
from configs.config import TrainingConfig

def generate_submission_csv(
    dataset_root: str,
    model_path: str,
    output_csv: str = "./submission.csv",
    cache_dir: str = "./eval_cache",
    seed: int = 42,
    top_k: int = 100,
    weight_cfg: Dict[str, float] = None,
    device: str = "cuda"
):
    """
    ç”ŸæˆKaggleæäº¤CSVæ–‡ä»¶
    
    Args:
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•
        model_path: æ¨¡å‹æƒé‡è·¯å¾„
        output_csv: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        cache_dir: ç¼“å­˜ç›®å½•
        seed: éšæœºç§å­
        top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›å‰Kä¸ªç»“æœ
        weight_cfg: æ¨¡æ€æƒé‡é…ç½®
        device: è®¡ç®—è®¾å¤‡
    """
    print("ğŸ¯ å¼€å§‹ç”ŸæˆKaggleæäº¤æ–‡ä»¶")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    torch.manual_seed(seed)
    np.random.seed(seed)
    rng = random.Random(seed)
    
    # é»˜è®¤æƒé‡é…ç½®
    if weight_cfg is None:
        weight_cfg = {"ir": 1.0, "cpencil": 1.0, "sketch": 1.0, "text": 1.2}
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(device if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    config = TrainingConfig()
    model = CLIPBasedMultiModalReIDModel(config).to(device)
    
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if 'num_classes' in checkpoint:
        model.set_num_classes(checkpoint['num_classes'])
    
    model.eval()
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = FeatureExtractor(model, device)
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½æ•°æ®é›†...")
    config.data_root = dataset_root
    dataset = MultiModalDataset(config, split='val')
    
    # æ„å»ºæ•°æ®ç´¢å¼•
    index = build_index(dataset)
    
    # æ„å»ºGallery
    gallery = build_gallery(index)
    print(f"Galleryå¤§å°: {len(gallery)} å¼ RGBå›¾åƒ")
    
    # æå–Galleryç‰¹å¾
    g_feats, g_meta = extract_gallery_feats(gallery, extractor, cache_dir)
    g_feats = l2n(g_feats)
    
    # æ”¶é›†æ‰€æœ‰æŸ¥è¯¢
    all_queries = []
    print("\næ„å»ºæ‰€æœ‰æŸ¥è¯¢...")
    
    for k in [1, 2, 3, 4]:
        queries = build_queries(index, mode_k=k, rng=rng, main_mod_choice="lexi_first")
        print(f"MM-{k}: {len(queries)} ä¸ªæŸ¥è¯¢")
        
        for q in queries:
            q['mode'] = f"MM-{k}"
            all_queries.append(q)
    
    print(f"æ€»æŸ¥è¯¢æ•°: {len(all_queries)}")
    
    # ç”Ÿæˆæäº¤æ•°æ®
    submission_data = []
    
    print("\nç”Ÿæˆæäº¤æ•°æ®...")
    for q in tqdm(all_queries, desc="å¤„ç†æŸ¥è¯¢"):
        try:
            # æå–æŸ¥è¯¢ç‰¹å¾
            q_feat = extract_query_feat(q, extractor, weight_cfg).view(1, -1)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            sims = cosine_sim(q_feat, g_feats).squeeze(0)  # [G]
            
            # æ’åºå¹¶å–å‰K
            ranks = torch.argsort(sims, descending=True)[:top_k].tolist()
            
            # ç”Ÿæˆquery_key
            pid = q["pid"]
            mode = q["mode"]
            mods = "+".join(sorted(q["modalities"]))
            
            # æ„å»ºæŸ¥è¯¢æ ·æœ¬ID
            sample_ids = []
            for sample in q["samples"].values():
                if "img_id" in sample and sample["img_id"] is not None:
                    sample_ids.append(str(sample["img_id"]))
            
            query_key = f"{pid}|{mode}|{mods}|{'+'.join(sample_ids)}"
            
            # è·å–æ’åºåçš„gallery img_id
            ranked_gallery_ids = []
            for rank_idx in ranks:
                g_item = g_meta[rank_idx]
                g_img_id = g_item.get("img_id", "unknown")
                if g_img_id is not None:
                    ranked_gallery_ids.append(str(g_img_id))
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç»“æœ
            while len(ranked_gallery_ids) < top_k:
                ranked_gallery_ids.append("unknown")
            
            ranked_gallery_ids_str = " ".join(ranked_gallery_ids[:top_k])
            
            submission_data.append({
                "query_key": query_key,
                "ranked_gallery_ids": ranked_gallery_ids_str
            })
            
        except Exception as e:
            print(f"å¤„ç†æŸ¥è¯¢å¤±è´¥ {q}: {e}")
            continue
    
    # ä¿å­˜CSV
    print(f"\nä¿å­˜æäº¤æ–‡ä»¶: {output_csv}")
    df = pd.DataFrame(submission_data)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(os.path.abspath(output_csv)), exist_ok=True)
    
    df.to_csv(output_csv, index=False)
    
    print(f"âœ… æäº¤æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
    print(f"   æ–‡ä»¶è·¯å¾„: {output_csv}")
    print(f"   æŸ¥è¯¢æ¡ç›®æ•°: {len(submission_data)}")
    print(f"   æ¯ä¸ªæŸ¥è¯¢è¿”å›: {top_k} ä¸ªç»“æœ")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    if len(submission_data) > 0:
        print(f"\nç¤ºä¾‹æäº¤æ¡ç›®:")
        for i, item in enumerate(submission_data[:3]):
            print(f"  {i+1}. Query: {item['query_key']}")
            gallery_preview = " ".join(item['ranked_gallery_ids'].split()[:5])
            print(f"     Gallery (å‰5): {gallery_preview}...")
    
    return output_csv

def validate_submission_format(csv_path: str):
    """
    éªŒè¯æäº¤CSVæ ¼å¼æ˜¯å¦æ­£ç¡®
    
    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
    """
    print(f"\nğŸ” éªŒè¯æäº¤æ–‡ä»¶æ ¼å¼: {csv_path}")
    
    try:
        df = pd.read_csv(csv_path)
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_cols = ["query_key", "ranked_gallery_ids"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"âŒ ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
            return False
        
        print(f"âœ… åˆ—åæ£€æŸ¥é€šè¿‡: {list(df.columns)}")
        
        # æ£€æŸ¥æ•°æ®
        print(f"âœ… æäº¤æ¡ç›®æ•°: {len(df)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå€¼
        null_queries = df['query_key'].isnull().sum()
        null_galleries = df['ranked_gallery_ids'].isnull().sum()
        
        if null_queries > 0:
            print(f"âš ï¸ å‘ç° {null_queries} ä¸ªç©ºæŸ¥è¯¢key")
        
        if null_galleries > 0:
            print(f"âš ï¸ å‘ç° {null_galleries} ä¸ªç©ºgalleryç»“æœ")
        
        # æ£€æŸ¥æŸ¥è¯¢keyæ ¼å¼
        sample_queries = df['query_key'].head(5).tolist()
        print(f"âœ… ç¤ºä¾‹æŸ¥è¯¢key:")
        for i, qk in enumerate(sample_queries):
            print(f"   {i+1}. {qk}")
        
        # æ£€æŸ¥galleryç»“æœæ ¼å¼
        sample_gallery = df['ranked_gallery_ids'].iloc[0]
        gallery_count = len(sample_gallery.split())
        print(f"âœ… æ¯ä¸ªæŸ¥è¯¢çš„galleryç»“æœæ•°: {gallery_count}")
        
        print(f"âœ… æäº¤æ–‡ä»¶æ ¼å¼éªŒè¯é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç”ŸæˆKaggleæäº¤CSV")
    parser.add_argument("--dataset_root", type=str, default="./data/train", help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--model_path", type=str, default="./checkpoints/best_model.pth", help="æ¨¡å‹æƒé‡è·¯å¾„")
    parser.add_argument("--output_csv", type=str, default="./submission.csv", help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--cache_dir", type=str, default="./eval_cache", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--top_k", type=int, default=100, help="æ¯ä¸ªæŸ¥è¯¢è¿”å›å‰Kä¸ªç»“æœ")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--validate", action="store_true", help="éªŒè¯ç”Ÿæˆçš„CSVæ ¼å¼")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.dataset_root):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {args.dataset_root}")
        sys.exit(1)
    
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)
    
    # ç”Ÿæˆæäº¤æ–‡ä»¶
    try:
        output_path = generate_submission_csv(
            dataset_root=args.dataset_root,
            model_path=args.model_path,
            output_csv=args.output_csv,
            cache_dir=args.cache_dir,
            seed=args.seed,
            top_k=args.top_k,
            device=args.device
        )
        
        # éªŒè¯æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.validate:
            validate_submission_format(output_path)
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæäº¤æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
