# tools/test_clip_mer_integration.py
"""
CLIP+MERæ¶æ„é›†æˆæµ‹è¯•è„šæœ¬
éªŒè¯æ–°æ¶æ„çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import logging
from configs.config import TrainingConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def test_clip_mer_integration():
    """æµ‹è¯•CLIP+MERæ¶æ„çš„åŸºæœ¬åŠŸèƒ½"""
    
    print("ğŸš€ å¼€å§‹CLIP+MERæ¶æ„é›†æˆæµ‹è¯•...")
    
    # 1. æµ‹è¯•é…ç½®åŠ è½½
    print("\nğŸ“‹ æµ‹è¯•1: é…ç½®åŠ è½½")
    try:
        config = TrainingConfig()
        
        # æ£€æŸ¥æ–°æ·»åŠ çš„é…ç½®é¡¹
        required_configs = [
            'clip_model_name', 'modalities', 'mer_lora_rank', 
            'base_learning_rate', 'mer_learning_rate'
        ]
        
        for cfg_name in required_configs:
            if hasattr(config, cfg_name):
                value = getattr(config, cfg_name)
                print(f"  âœ… {cfg_name}: {value}")
            else:
                print(f"  âŒ ç¼ºå°‘é…ç½®é¡¹: {cfg_name}")
                return False
        
        print("  âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"  âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False
    
    # 2. æµ‹è¯•æ¨¡å‹å¯¼å…¥å’Œå®ä¾‹åŒ–
    print("\nğŸ—ï¸ æµ‹è¯•2: æ¨¡å‹å¯¼å…¥å’Œå®ä¾‹åŒ–")
    try:
        from models.model import CLIPBasedMultiModalReIDModel
        
        # åˆ›å»ºæ¨¡å‹
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        config.device = device
        
        model = CLIPBasedMultiModalReIDModel(config)
        model = model.to(device)
        
        # è®¾ç½®åˆ†ç±»å™¨
        model.set_num_classes(100)  # å‡è®¾100ä¸ªID
        
        print(f"  âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè®¾å¤‡: {device}")
        print(f"  âœ… æ”¯æŒæ¨¡æ€: {model.modalities}")
        print(f"  âœ… èåˆç»´åº¦: {model.fusion_dim}")
        
    except Exception as e:
        print(f"  âŒ æ¨¡å‹å®ä¾‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 3. æµ‹è¯•å‰å‘ä¼ æ’­
    print("\nâš¡ æµ‹è¯•3: å‰å‘ä¼ æ’­")
    try:
        batch_size = 4
        
        # å‡†å¤‡æµ‹è¯•æ•°æ® (æ³¨æ„é€šé“æ•°åŒ¹é…)
        test_images = {
            'rgb': torch.randn(batch_size, 3, 224, 224, device=device),      # RGB: 3é€šé“
            'ir': torch.randn(batch_size, 1, 224, 224, device=device),       # IR: 1é€šé“  
            'cpencil': torch.randn(batch_size, 3, 224, 224, device=device),  # å½©é“…: 3é€šé“
            'sketch': torch.randn(batch_size, 1, 224, 224, device=device),   # ç´ æ: 1é€šé“
        }
        
        test_texts = [
            "A person walking in the street",
            "ä¸€ä¸ªè¡Œäººåœ¨è·¯ä¸Šè¡Œèµ°",
            "Person wearing red jacket",
            "ç©¿çº¢è‰²å¤–å¥—çš„äºº"
        ]
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            # æµ‹è¯•è§†è§‰+æ–‡æœ¬
            outputs = model(images=test_images, texts=test_texts)
            print(f"  âœ… è§†è§‰+æ–‡æœ¬å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"    - Features shape: {outputs['features'].shape}")
            print(f"    - Logits shape: {outputs['logits'].shape}")
            
            # æµ‹è¯•ä»…è§†è§‰ (ä»…æµ‹è¯•RGBæ¨¡æ€)
            outputs_vis = model(images={'rgb': test_images['rgb']}, texts=None)
            print(f"  âœ… ä»…è§†è§‰å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"    - Features shape: {outputs_vis['features'].shape}")
            
            # æµ‹è¯•å•ä¸ªIRæ¨¡æ€
            outputs_ir = model(images={'ir': test_images['ir']}, texts=None)
            print(f"  âœ… IRæ¨¡æ€å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"    - Features shape: {outputs_ir['features'].shape}")
            
            # æµ‹è¯•ä»…æ–‡æœ¬
            outputs_text = model(images=None, texts=test_texts)
            print(f"  âœ… ä»…æ–‡æœ¬å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"    - Features shape: {outputs_text['features'].shape}")
        
    except Exception as e:
        print(f"  âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 4. æµ‹è¯•æŸå¤±è®¡ç®—
    print("\nğŸ’¸ æµ‹è¯•4: æŸå¤±è®¡ç®—")
    try:
        # å‡†å¤‡æ ‡ç­¾
        labels = torch.randint(0, 100, (batch_size,), device=device)
        
        # è®¡ç®—æŸå¤±
        loss_dict = model.compute_loss(outputs, labels)
        
        required_losses = ['total_loss', 'ce_loss', 'sdm_loss', 'feat_penalty']
        for loss_name in required_losses:
            if loss_name in loss_dict:
                loss_value = loss_dict[loss_name].item()
                print(f"  âœ… {loss_name}: {loss_value:.4f}")
            else:
                print(f"  âŒ ç¼ºå°‘æŸå¤±é¡¹: {loss_name}")
                return False
        
    except Exception as e:
        print(f"  âŒ æŸå¤±è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 5. æµ‹è¯•å‚æ•°åˆ†ç»„
    print("\nâš™ï¸ æµ‹è¯•5: å‚æ•°åˆ†ç»„")
    try:
        param_groups = model.get_learnable_params()
        
        total_params = 0
        for group in param_groups:
            group_name = group.get('name', 'unknown')
            group_lr = group['lr']
            num_params = len(group['params'])
            total_params += num_params
            print(f"  âœ… {group_name}: {num_params} å‚æ•°, LR: {group_lr:.2e}")
        
        print(f"  âœ… æ€»å‚æ•°ç»„æ•°: {len(param_groups)}, æ€»å‚æ•°æ•°: {total_params}")
        
    except Exception as e:
        print(f"  âŒ å‚æ•°åˆ†ç»„å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 6. æµ‹è¯•è®­ç»ƒè„šæœ¬çš„å…¼å®¹æ€§å‡½æ•°
    print("\nğŸ”„ æµ‹è¯•6: è®­ç»ƒè„šæœ¬å…¼å®¹æ€§")
    try:
        # å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„è¾…åŠ©å‡½æ•°
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from train import convert_batch_for_clip_model, call_model_with_batch, move_batch_to_device
        
        # æ¨¡æ‹Ÿæ•°æ®é›†batchæ ¼å¼
        mock_batch = {
            'images': {
                'vis': torch.randn(2, 3, 224, 224),
                'nir': torch.randn(2, 1, 224, 224),
            },
            'text_description': ['Person walking', 'A man in blue shirt'],
            'person_id': torch.tensor([1, 2])
        }
        
        # æµ‹è¯•batchè½¬æ¢
        images, texts = convert_batch_for_clip_model(mock_batch)
        print(f"  âœ… Batchè½¬æ¢æˆåŠŸ: {list(images.keys())} + {len(texts) if texts else 0} texts")
        
        # æµ‹è¯•è®¾å¤‡ç§»åŠ¨
        mock_batch = move_batch_to_device(mock_batch, device)
        print(f"  âœ… è®¾å¤‡ç§»åŠ¨æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹è°ƒç”¨
        outputs = call_model_with_batch(model, mock_batch, return_features=True)
        print(f"  âœ… å…¼å®¹æ€§æ¨¡å‹è°ƒç”¨æˆåŠŸ: {outputs['features'].shape}")
        
    except Exception as e:
        print(f"  âŒ è®­ç»ƒè„šæœ¬å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CLIP+MERæ¶æ„é›†æˆæˆåŠŸï¼")
    print("\nğŸ“ æ¶æ„æ€»ç»“:")
    print("  - âœ… CLIP-B/16ç»Ÿä¸€ç¼–ç å™¨")
    print("  - âœ… å¤šæ¨¡æ€éå…±äº«tokenizer (RGB/IR/cpencil/sketch/text)")
    print("  - âœ… MERæ¨¡æ€è·¯ç”±LoRA (rank=4)")
    print("  - âœ… SDMè¯­ä¹‰åˆ†ç¦»æ¨¡å—")
    print("  - âœ… ç‰¹å¾èåˆå™¨")
    print("  - âœ… åˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–")
    print("  - âœ… è®­ç»ƒè„šæœ¬å…¼å®¹æ€§")
    
    return True


if __name__ == "__main__":
    success = test_clip_mer_integration()
    if success:
        print("\nâœ¨ å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼è¿è¡Œ: python train.py")
    else:
        print("\nâŒ è¯·ä¿®å¤ä¸Šè¿°é”™è¯¯åå†ç»§ç»­")
        sys.exit(1)
