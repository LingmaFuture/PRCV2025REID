# test_conservative_scheduler.py
"""
æµ‹è¯•ä¿å®ˆå­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
éªŒè¯ä¸åŒè°ƒåº¦å™¨ç±»å‹çš„å­¦ä¹ ç‡å˜åŒ–æ›²çº¿
"""

import torch
import matplotlib.pyplot as plt
import numpy as np
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, MultiStepLR, ReduceLROnPlateau
from torch.optim.lr_scheduler import LinearLR, SequentialLR

def create_dummy_model():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ç”¨äºæµ‹è¯•"""
    return torch.nn.Linear(10, 1)

def test_scheduler_curves():
    """æµ‹è¯•ä¸åŒè°ƒåº¦å™¨çš„å­¦ä¹ ç‡å˜åŒ–æ›²çº¿"""
    
    # åŸºç¡€å‚æ•°
    base_lr = 5e-4
    num_epochs = 100
    warmup_epochs = 15
    conservative_factor = 0.7
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = create_dummy_model()
    
    # æµ‹è¯•ä¸åŒè°ƒåº¦å™¨
    schedulers_config = {
        'cosine_conservative': {
            'type': 'cosine',
            'warmup': True,
            'eta_min': base_lr * 0.01
        },
        'step_conservative': {
            'type': 'step',
            'step_size': int(50 * conservative_factor),
            'gamma': 0.3 + 0.4 * conservative_factor
        },
        'multistep_conservative': {
            'type': 'multistep',
            'milestones': [int(60 * conservative_factor), int(80 * conservative_factor), int(95 * conservative_factor)],
            'gamma': 0.2 + 0.5 * conservative_factor
        },
        'plateau_conservative': {
            'type': 'plateau',
            'factor': 0.5,
            'patience': 8
        }
    }
    
    plt.figure(figsize=(15, 10))
    
    for i, (name, config) in enumerate(schedulers_config.items(), 1):
        plt.subplot(2, 2, i)
        
        # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = AdamW(model.parameters(), lr=base_lr)
        
        # åˆ›å»ºè°ƒåº¦å™¨
        if config['type'] == 'cosine':
            if config.get('warmup', False):
                warmup_scheduler = LinearLR(optimizer, start_factor=0.01, total_iters=warmup_epochs)
                cosine_scheduler = CosineAnnealingLR(
                    optimizer, 
                    T_max=num_epochs - warmup_epochs, 
                    eta_min=config['eta_min']
                )
                scheduler = SequentialLR(
                    optimizer, 
                    [warmup_scheduler, cosine_scheduler], 
                    milestones=[warmup_epochs]
                )
            else:
                scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=config['eta_min'])
                
        elif config['type'] == 'step':
            scheduler = StepLR(optimizer, step_size=config['step_size'], gamma=config['gamma'])
            
        elif config['type'] == 'multistep':
            scheduler = MultiStepLR(optimizer, milestones=config['milestones'], gamma=config['gamma'])
            
        elif config['type'] == 'plateau':
            scheduler = ReduceLROnPlateau(
                optimizer, 
                mode='max', 
                factor=config['factor'], 
                patience=config['patience'],
                min_lr=base_lr * 0.001
            )
        
        # è®°å½•å­¦ä¹ ç‡å˜åŒ–
        learning_rates = []
        epochs = list(range(1, num_epochs + 1))
        
        for epoch in epochs:
            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            learning_rates.append(current_lr)
            
            # æ­¥è¿›è°ƒåº¦å™¨
            if config['type'] == 'plateau':
                # æ¨¡æ‹ŸmAPå˜åŒ–ï¼ˆå…ˆä¸Šå‡åå¹³ç¨³ï¼‰
                if epoch < 30:
                    fake_map = 0.1 + epoch * 0.01
                elif epoch < 60:
                    fake_map = 0.4 + (epoch - 30) * 0.005
                else:
                    fake_map = 0.55 + np.random.normal(0, 0.01)  # æ·»åŠ å™ªå£°æ¨¡æ‹Ÿæ³¢åŠ¨
                scheduler.step(fake_map)
            else:
                scheduler.step()
        
        # ç»˜åˆ¶æ›²çº¿
        plt.plot(epochs, learning_rates, linewidth=2, label=f'{name}')
        plt.title(f'{name.upper()} å­¦ä¹ ç‡è°ƒåº¦å™¨\n'
                 f'é…ç½®: {config}', fontsize=10)
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.grid(True, alpha=0.3)
        plt.yscale('log')
        
        # æ·»åŠ å…³é”®ä¿¡æ¯
        min_lr = min(learning_rates)
        max_lr = max(learning_rates)
        plt.text(0.02, 0.98, f'Max LR: {max_lr:.2e}\nMin LR: {min_lr:.2e}', 
                transform=plt.gca().transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig('tools/conservative_scheduler_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ä¿å®ˆè°ƒåº¦å™¨æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š å­¦ä¹ ç‡å˜åŒ–æ›²çº¿å·²ä¿å­˜åˆ°: tools/conservative_scheduler_comparison.png")
    print(f"ğŸ“‹ è°ƒåº¦å™¨é…ç½®å¯¹æ¯”:")
    for name, config in schedulers_config.items():
        print(f"  - {name}: {config}")

def validate_scheduler_stability():
    """éªŒè¯è°ƒåº¦å™¨ç¨³å®šæ€§å‚æ•°"""
    
    base_lr = 5e-4
    conservative_factor = 0.7
    
    print("\nğŸ” ä¿å®ˆè°ƒåº¦å™¨ç¨³å®šæ€§å‚æ•°éªŒè¯:")
    print(f"åŸºç¡€å­¦ä¹ ç‡: {base_lr:.2e}")
    print(f"ä¿å®ˆç³»æ•°: {conservative_factor}")
    
    # Cosineè°ƒåº¦å™¨
    eta_min = base_lr * 0.01
    print(f"\nğŸ“ˆ Cosineè°ƒåº¦å™¨:")
    print(f"  - æœ€å°å­¦ä¹ ç‡: {eta_min:.2e} (åŸºç¡€LRçš„1%)")
    print(f"  - Warmupå¯åŠ¨å› å­: 0.01 (æ›´æ¸©å’Œçš„å¯åŠ¨)")
    
    # Stepè°ƒåº¦å™¨
    step_size = int(50 * conservative_factor)
    gamma = 0.3 + 0.4 * conservative_factor
    print(f"\nğŸ“ˆ Stepè°ƒåº¦å™¨:")
    print(f"  - æ­¥é•¿: {step_size} epochs")
    print(f"  - è¡°å‡å› å­: {gamma:.2f}")
    
    # MultiStepè°ƒåº¦å™¨
    milestones = [int(60 * conservative_factor), int(80 * conservative_factor), int(95 * conservative_factor)]
    gamma_multi = 0.2 + 0.5 * conservative_factor
    print(f"\nğŸ“ˆ MultiStepè°ƒåº¦å™¨:")
    print(f"  - é‡Œç¨‹ç¢‘: {milestones}")
    print(f"  - è¡°å‡å› å­: {gamma_multi:.2f}")
    
    # Plateauè°ƒåº¦å™¨
    print(f"\nğŸ“ˆ Plateauè°ƒåº¦å™¨:")
    print(f"  - è¡°å‡å› å­: 0.5")
    print(f"  - è€å¿ƒå€¼: 8 epochs")
    print(f"  - æœ€å°å­¦ä¹ ç‡: {base_lr * 0.001:.2e}")
    
    print(f"\nâœ… æ‰€æœ‰å‚æ•°å‡å·²è®¾ç½®ä¸ºæ›´ä¿å®ˆçš„å€¼ï¼Œæœ‰åŠ©äºæå‡è®­ç»ƒç¨³å®šæ€§ï¼")

if __name__ == "__main__":
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ä¿å®ˆå­¦ä¹ ç‡è°ƒåº¦å™¨...")
    
    try:
        test_scheduler_curves()
        validate_scheduler_stability()
        
        print(f"\nğŸ‰ ä¿å®ˆè°ƒåº¦å™¨æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ å»ºè®®ä½¿ç”¨çš„è°ƒåº¦å™¨ç±»å‹:")
        print(f"  1. ä¸€èˆ¬æƒ…å†µ: cosine (å¸¦warmup)")
        print(f"  2. è®­ç»ƒä¸ç¨³å®š: plateau (è‡ªé€‚åº”)")
        print(f"  3. éœ€è¦é˜¶æ®µæ€§è¡°å‡: multistep")
        
    except ImportError as e:
        print(f"âš ï¸  ç¼ºå°‘ä¾èµ–åº“: {e}")
        print(f"ğŸ’¡ è¯·å®‰è£…: pip install matplotlib")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
