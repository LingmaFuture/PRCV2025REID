#!/usr/bin/env python3
# test_performance_optimization.py
# å¿«é€ŸéªŒè¯æ€§èƒ½ä¼˜åŒ–æ•ˆæœ

import torch
import time
import sys
from datasets.dataset import MultiModalDataset, compatible_collate_fn
from tools.cached_sampler import CachedModalAwarePKSampler
from torch.utils.data import DataLoader
from configs.config import TrainingConfig
from tools.split import split_ids, create_split_datasets

def test_performance_optimization():
    """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–çš„å„ä¸ªç»„ä»¶"""
    
    print("ğŸš€ æ€§èƒ½ä¼˜åŒ–æµ‹è¯•")
    print("=" * 50)
    
    try:
        # 1. æµ‹è¯•ç¼“å­˜é‡‡æ ·å™¨å¯¼å…¥
        print("âœ… 1/5: ç¼“å­˜é‡‡æ ·å™¨å¯¼å…¥æˆåŠŸ")
        
        # 2. åŠ è½½æ•°æ®é›†
        print("ğŸ“‚ 2/5: åŠ è½½æ•°æ®é›†...")
        config = TrainingConfig()
        full_dataset = MultiModalDataset(config, split='train')
        
        # æ•°æ®é›†åˆ’åˆ†
        all_person_ids = [full_dataset.data_list[i]['person_id'] for i in range(len(full_dataset))]
        all_person_ids = sorted(list(set(all_person_ids)))
        train_ids, val_ids = split_ids(all_person_ids, val_ratio=0.2, seed=42)
        train_dataset, _ = create_split_datasets(full_dataset, train_ids, val_ids, config)
        
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, {len(train_ids)} ID")
        print("âœ… 2/5: æ•°æ®é›†åŠ è½½æˆåŠŸ")
        
        # 3. æµ‹è¯•ç¼“å­˜é‡‡æ ·å™¨åˆ›å»ºå’Œç¼“å­˜æ—¶é—´
        print("â±ï¸  3/5: æµ‹è¯•ç¼“å­˜é‡‡æ ·å™¨æ€§èƒ½...")
        actual_batch_size = 32
        num_instances = 4
        
        cache_start = time.time()
        train_sampler = CachedModalAwarePKSampler(
            dataset=train_dataset,
            batch_size=actual_batch_size,
            num_instances=num_instances,
            ensure_rgb=True,
            prefer_complete=True,
            seed=42,
        )
        cache_time = time.time() - cache_start
        
        print(f"   ç¼“å­˜æ—¶é—´: {cache_time:.2f}s (ç›®æ ‡: <5s)")
        print(f"   å¯é…å¯¹ID: {len(train_sampler.pids_pairable)}/{len(train_sampler.pids_all)} ({len(train_sampler.pids_pairable)/len(train_sampler.pids_all):.1%})")
        print(f"   é¢„æœŸbatchæ•°: {len(train_sampler)}")
        
        if cache_time > 5.0:
            print("   âš ï¸ ç¼“å­˜æ—¶é—´åé•¿ï¼Œä½†ä»åœ¨å¯æ¥å—èŒƒå›´")
        else:
            print("   âœ… ç¼“å­˜æ—¶é—´ç†æƒ³")
            
        print("âœ… 3/5: ç¼“å­˜é‡‡æ ·å™¨åˆ›å»ºæˆåŠŸ")
        
        # 4. æµ‹è¯•ä¼˜åŒ–åçš„DataLoader
        print("ğŸ”§ 4/5: æµ‹è¯•ä¼˜åŒ–DataLoader...")
        
        train_loader = DataLoader(
            train_dataset,
            batch_sampler=train_sampler,
            num_workers=2,                    # Windowsä¼˜åŒ–
            pin_memory=True,                  # é…åˆnon_blocking
            persistent_workers=True,          # é¿å…é‡å¤spawn
            prefetch_factor=1,               # é™ä½å†…å­˜äº‰ç”¨
            drop_last=True,                  # é¿å…åŠ¨æ€shape
            collate_fn=compatible_collate_fn
        )
        
        print("âœ… 4/5: ä¼˜åŒ–DataLoaderåˆ›å»ºæˆåŠŸ")
        
        # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
        print("âš¡ 5/5: è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æµ‹è¯•æ•°æ®åŠ è½½é€Ÿåº¦
        test_batches = min(10, len(train_loader))
        load_times = []
        
        print(f"   æµ‹è¯• {test_batches} ä¸ªbatchçš„åŠ è½½é€Ÿåº¦...")
        
        for batch_idx, batch in enumerate(train_loader):
            if batch_idx >= test_batches:
                break
                
            batch_start = time.time()
            
            # æ¨¡æ‹Ÿæ•°æ®ä¼ è¾“åˆ°GPU
            def move_to_device(obj):
                if isinstance(obj, dict):
                    return {k: move_to_device(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [move_to_device(x) for x in obj]
                elif torch.is_tensor(obj):
                    return obj.to(device, non_blocking=True)
                else:
                    return obj
                    
            batch = move_to_device(batch)
            
            # åŒæ­¥ç­‰å¾…ä¼ è¾“å®Œæˆ
            if device.type == 'cuda':
                torch.cuda.synchronize()
                
            batch_time = time.time() - batch_start
            load_times.append(batch_time)
            
            # éªŒè¯batchå†…å®¹
            labels = batch['person_id']
            modality_mask = batch.get('modality_mask', {})
            
            # ç»Ÿè®¡å¯é…å¯¹æ€§
            unique_ids = torch.unique(labels)
            pairable_count = 0
            
            for pid in unique_ids:
                pid_indices = (labels == pid)
                has_vis = modality_mask.get('vis', torch.zeros_like(labels))[pid_indices].any()
                has_non_vis = any(modality_mask.get(m, torch.zeros_like(labels))[pid_indices].any() 
                                 for m in ['nir', 'sk', 'cp', 'text'])
                if has_vis and has_non_vis:
                    pairable_count += 1
            
            pairable_ratio = pairable_count / len(unique_ids) if len(unique_ids) > 0 else 0
            
            if batch_idx < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªbatchçš„è¯¦æƒ…
                print(f"     Batch {batch_idx}: {len(labels)}æ ·æœ¬, {len(unique_ids)}ID, "
                      f"å¯é…å¯¹{pairable_count}/{len(unique_ids)} ({pairable_ratio:.1%}), "
                      f"ç”¨æ—¶{batch_time:.3f}s")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_batch_time = sum(load_times) / len(load_times)
        batches_per_sec = 1.0 / avg_batch_time
        samples_per_sec = actual_batch_size * batches_per_sec
        
        print(f"\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(f"   å¹³å‡batchæ—¶é—´: {avg_batch_time:.3f}s")
        print(f"   æ‰¹æ¬¡å¤„ç†é€Ÿåº¦: {batches_per_sec:.2f} batches/s")
        print(f"   æ ·æœ¬å¤„ç†é€Ÿåº¦: {samples_per_sec:.2f} samples/s")
        print(f"   ç­‰æ•ˆit/s: {batches_per_sec:.2f} it/s")
        
        # æ€§èƒ½è¯„ä¼°
        if batches_per_sec >= 8.0:
            print("   ğŸ‰ æ€§èƒ½ä¼˜ç§€! è¾¾åˆ°ä¼˜åŒ–ç›®æ ‡ (>8 it/s)")
            success_level = "ä¼˜ç§€"
        elif batches_per_sec >= 4.0:
            print("   âœ… æ€§èƒ½è‰¯å¥½! æ˜¾è‘—ä¼˜äºä¼˜åŒ–å‰ (~2xæå‡)")  
            success_level = "è‰¯å¥½"
        elif batches_per_sec >= 2.5:
            print("   âš ï¸ æ€§èƒ½ä¸€èˆ¬ï¼Œæœ‰ä¸€å®šæå‡ä½†æœªè¾¾åˆ°é¢„æœŸ")
            success_level = "ä¸€èˆ¬"
        else:
            print("   âŒ æ€§èƒ½æå‡ä¸æ˜æ˜¾ï¼Œéœ€è¦è¿›ä¸€æ­¥è¯Šæ–­")
            success_level = "éœ€è¦ä¼˜åŒ–"
            
        print("âœ… 5/5: æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        
        # æ€»ç»“
        print(f"\nğŸ¯ ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
        print(f"   ç¼“å­˜æ—¶é—´: {cache_time:.2f}s")
        print(f"   å¯é…å¯¹ç‡: {len(train_sampler.pids_pairable)/len(train_sampler.pids_all):.1%}")
        print(f"   å¤„ç†é€Ÿåº¦: {batches_per_sec:.2f} it/s")
        print(f"   æ€§èƒ½ç­‰çº§: {success_level}")
        
        if success_level in ["ä¼˜ç§€", "è‰¯å¥½"]:
            print(f"\nğŸš€ ä¼˜åŒ–æˆåŠŸ! å¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒ:")
            print(f"   python train.py")
            return True
        else:
            print(f"\nğŸ” å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–:")
            print(f"   1. æ£€æŸ¥ç¡¬ä»¶é…ç½® (SSD, è¶³å¤Ÿå†…å­˜)")
            print(f"   2. è°ƒæ•´batch_size (å¦‚æœæ˜¾å­˜å…è®¸)")
            print(f"   3. å°è¯•num_workers=0 æˆ– 1")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_performance_optimization()
    
    print("\n" + "=" * 50)
    if success:
        print("ğŸ‰ æ€§èƒ½ä¼˜åŒ–æµ‹è¯•é€šè¿‡! å¯ä»¥å¼€å§‹è®­ç»ƒäº†!")
        print("è¿è¡Œ: python train.py")
    else:
        print("ğŸ”§ æ€§èƒ½ä¼˜åŒ–éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")
        print("æŸ¥çœ‹: PERFORMANCE_OPTIMIZATION_GUIDE.md")
        
    sys.exit(0 if success else 1)